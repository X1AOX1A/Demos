WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
| distributed init (rank 2, world 4): env://
| distributed init (rank 3, world 4): env://
| distributed init (rank 1, world 4): env://
| distributed init (rank 0, world 4): env://
2023-05-30 20:02:57,516 [INFO] 
=====  Running Parameters    =====
2023-05-30 20:02:57,517 [INFO] {
    "accum_grad_iters": 1,
    "amp": true,
    "batch_size_eval": 8,
    "batch_size_train": 16,
    "device": "cuda",
    "dist_backend": "nccl",
    "dist_url": "env://",
    "distributed": true,
    "evaluate": false,
    "gpu": 0,
    "init_lr": 1e-05,
    "lr_sched": "linear_warmup_cosine_lr",
    "max_epoch": 5,
    "max_len": 30,
    "min_len": 8,
    "min_lr": 0,
    "num_beams": 5,
    "num_workers": 4,
    "output_dir": "/root/Documents/DEMOS/InstructBLIP/output/BLIP2_Instruct/Caption_coco",
    "rank": 0,
    "resume_ckpt_path": null,
    "seed": 42,
    "task": "captioning",
    "test_splits": [
        "test"
    ],
    "train_splits": [
        "train"
    ],
    "valid_splits": [
        "val"
    ],
    "warmup_lr": 1e-08,
    "warmup_steps": 1000,
    "weight_decay": 0.05,
    "world_size": 4
}
2023-05-30 20:02:57,517 [INFO] 
======  Dataset Attributes  ======
2023-05-30 20:02:57,518 [INFO] 
======== coco_caption_extend =======
2023-05-30 20:02:57,518 [INFO] {
    "build_info": {
        "annotations": {
            "test": {
                "md5": "3ff34b0ef2db02d01c37399f6a2a6cd1",
                "storage": "/root/Documents/DATASETS/MS_COCO/annotations/coco_karpathy_test.json",
                "url": "https://storage.googleapis.com/sfr-vision-language-research/datasets/coco_karpathy_test.json"
            },
            "train": {
                "md5": "aa31ac474cf6250ebb81d18348a07ed8",
                "storage": "/root/Documents/DATASETS/MS_COCO/annotations/coco_karpathy_train.json",
                "url": "https://storage.googleapis.com/sfr-vision-language-research/datasets/coco_karpathy_train.json"
            },
            "val": {
                "md5": "b273847456ef5580e33713b1f7de52a0",
                "storage": "/root/Documents/DATASETS/MS_COCO/annotations/coco_karpathy_val.json",
                "url": "https://storage.googleapis.com/sfr-vision-language-research/datasets/coco_karpathy_val.json"
            }
        },
        "images": {
            "storage": "/root/Documents/DATASETS/MS_COCO/images/"
        }
    },
    "data_type": "images",
    "dataset_card": "dataset_card/coco_caption.md",
    "text_processor": {
        "eval": {
            "name": "blip_caption"
        },
        "train": {
            "name": "blip_caption"
        }
    },
    "vis_processor": {
        "eval": {
            "image_size": 364,
            "name": "blip_image_eval"
        },
        "train": {
            "image_size": 364,
            "name": "blip2_image_train"
        }
    }
}
2023-05-30 20:02:57,518 [INFO] 
======  Model Attributes  ======
2023-05-30 20:02:57,518 [INFO] {
    "arch": "blip2_vicuna_instruct",
    "drop_path_rate": 0,
    "finetuned": "",
    "freeze_vit": true,
    "image_size": 364,
    "llm_model": "/root/Documents/MODELS/Vicuna/7B",
    "load_finetuned": false,
    "load_pretrained": true,
    "model_type": "vicuna7b",
    "num_query_token": 32,
    "pretrained": "https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/InstructBLIP/instruct_blip_vicuna7b_trimmed.pth",
    "prompt": "",
    "use_grad_checkpoint": true,
    "vit_precision": "fp32"
}
Using downloaded and verified file: /root/Documents/DATASETS/MS_COCO/annotations/coco_karpathy_train.json
Using downloaded and verified file: /root/Documents/DATASETS/MS_COCO/annotations/coco_karpathy_val.json
Using downloaded and verified file: /root/Documents/DATASETS/MS_COCO/annotations/coco_karpathy_test.json
2023-05-30 20:02:57,519 [INFO] Building datasets...
Position interpolate from 16x16 to 26x26
2023-05-30 20:03:23,004 [INFO] freeze vision encoder
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.90s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.80s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.45s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.69s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.06s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.64s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.03s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.60s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.21s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.85s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  6.38s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  7.03s/it]
2023-05-30 20:06:07,508 [INFO] load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/InstructBLIP/instruct_blip_vicuna7b_trimmed.pth
2023-05-30 20:06:07,527 [INFO] Start training
2023-05-30 20:06:15,861 [INFO] dataset_ratios not specified, datasets will be concatenated (map-style datasets) or chained (webdataset.DataPipeline).
2023-05-30 20:06:15,862 [INFO] Loaded 566747 records for train split from the dataset.
2023-05-30 20:06:15,862 [INFO] Loaded 5000 records for val split from the dataset.
2023-05-30 20:06:15,862 [INFO] Loaded 5000 records for test split from the dataset.
2023-05-30 20:06:15,877 [INFO] number of trainable parameters: 188837376
2023-05-30 20:06:15,878 [INFO] Start training epoch 0, 8855 iters per inner epoch.
/root/anaconda3/envs/lavis/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/root/anaconda3/envs/lavis/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/root/anaconda3/envs/lavis/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/root/anaconda3/envs/lavis/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
Train: data epoch: [0]  [   0/8855]  eta: 9:45:48  lr: 0.000000  loss: 1.8588  time: 3.9693  data: 0.0000  max mem: 25719
2023-05-30 20:06:19,852 [INFO] Reducer buckets have been rebuilt in this iteration.
Train: data epoch: [0]  [  50/8855]  eta: 1:29:53  lr: 0.000001  loss: 1.4431  time: 0.5480  data: 0.0000  max mem: 30036
Train: data epoch: [0]  [ 100/8855]  eta: 1:24:21  lr: 0.000001  loss: 1.2807  time: 0.5470  data: 0.0000  max mem: 30036
Train: data epoch: [0]  [ 150/8855]  eta: 1:22:25  lr: 0.000002  loss: 1.3561  time: 0.5460  data: 0.0000  max mem: 30036
Train: data epoch: [0]  [ 200/8855]  eta: 1:21:02  lr: 0.000002  loss: 1.2218  time: 0.5455  data: 0.0000  max mem: 30036
Train: data epoch: [0]  [ 250/8855]  eta: 1:20:16  lr: 0.000003  loss: 1.7255  time: 0.5521  data: 0.0000  max mem: 31478
Train: data epoch: [0]  [ 300/8855]  eta: 1:19:33  lr: 0.000003  loss: 1.2979  time: 0.5489  data: 0.0000  max mem: 31478
Train: data epoch: [0]  [ 350/8855]  eta: 1:18:57  lr: 0.000004  loss: 1.3043  time: 0.5510  data: 0.0000  max mem: 32706
Train: data epoch: [0]  [ 400/8855]  eta: 1:18:20  lr: 0.000004  loss: 1.4207  time: 0.5453  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [ 450/8855]  eta: 1:17:50  lr: 0.000005  loss: 1.3626  time: 0.5542  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [ 500/8855]  eta: 1:17:15  lr: 0.000005  loss: 1.3740  time: 0.5451  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [ 550/8855]  eta: 1:16:44  lr: 0.000006  loss: 1.3721  time: 0.5449  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [ 600/8855]  eta: 1:16:16  lr: 0.000006  loss: 1.3116  time: 0.5517  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [ 650/8855]  eta: 1:15:48  lr: 0.000007  loss: 1.3361  time: 0.5511  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [ 700/8855]  eta: 1:15:16  lr: 0.000007  loss: 1.3425  time: 0.5458  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [ 750/8855]  eta: 1:14:42  lr: 0.000008  loss: 1.4487  time: 0.5422  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [ 800/8855]  eta: 1:14:11  lr: 0.000008  loss: 1.3377  time: 0.5442  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [ 850/8855]  eta: 1:13:44  lr: 0.000009  loss: 1.4721  time: 0.5480  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [ 900/8855]  eta: 1:13:16  lr: 0.000009  loss: 1.5683  time: 0.5509  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [ 950/8855]  eta: 1:12:47  lr: 0.000010  loss: 1.2500  time: 0.5463  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [1000/8855]  eta: 1:12:18  lr: 0.000010  loss: 1.3380  time: 0.5495  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [1050/8855]  eta: 1:11:49  lr: 0.000010  loss: 1.3595  time: 0.5497  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [1100/8855]  eta: 1:11:20  lr: 0.000010  loss: 1.8085  time: 0.5473  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [1150/8855]  eta: 1:10:52  lr: 0.000010  loss: 1.4704  time: 0.5482  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [1200/8855]  eta: 1:10:24  lr: 0.000010  loss: 1.8705  time: 0.5508  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [1250/8855]  eta: 1:09:56  lr: 0.000010  loss: 1.3146  time: 0.5542  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [1300/8855]  eta: 1:09:27  lr: 0.000010  loss: 1.7489  time: 0.5442  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [1350/8855]  eta: 1:08:58  lr: 0.000010  loss: 1.5773  time: 0.5441  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [1400/8855]  eta: 1:08:28  lr: 0.000010  loss: 1.2963  time: 0.5486  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [1450/8855]  eta: 1:07:59  lr: 0.000010  loss: 1.3460  time: 0.5396  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [1500/8855]  eta: 1:07:31  lr: 0.000010  loss: 1.3002  time: 0.5503  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [1550/8855]  eta: 1:07:02  lr: 0.000010  loss: 1.3753  time: 0.5469  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [1600/8855]  eta: 1:06:32  lr: 0.000010  loss: 1.8449  time: 0.5379  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [1650/8855]  eta: 1:06:04  lr: 0.000010  loss: 1.5475  time: 0.5529  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [1700/8855]  eta: 1:05:36  lr: 0.000010  loss: 1.3543  time: 0.5540  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [1750/8855]  eta: 1:05:08  lr: 0.000010  loss: 1.3789  time: 0.5376  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [1800/8855]  eta: 1:04:40  lr: 0.000010  loss: 1.2972  time: 0.5421  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [1850/8855]  eta: 1:04:13  lr: 0.000010  loss: 1.5324  time: 0.5477  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [1900/8855]  eta: 1:03:45  lr: 0.000010  loss: 1.4495  time: 0.5511  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [1950/8855]  eta: 1:03:17  lr: 0.000010  loss: 1.2521  time: 0.5529  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [2000/8855]  eta: 1:02:49  lr: 0.000010  loss: 1.5030  time: 0.5462  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [2050/8855]  eta: 1:02:20  lr: 0.000010  loss: 1.5542  time: 0.5441  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [2100/8855]  eta: 1:01:53  lr: 0.000010  loss: 1.6363  time: 0.5475  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [2150/8855]  eta: 1:01:24  lr: 0.000010  loss: 1.4239  time: 0.5446  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [2200/8855]  eta: 1:00:57  lr: 0.000010  loss: 1.2663  time: 0.5460  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [2250/8855]  eta: 1:00:28  lr: 0.000010  loss: 1.3294  time: 0.5418  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [2300/8855]  eta: 1:00:01  lr: 0.000010  loss: 1.4244  time: 0.5530  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [2350/8855]  eta: 0:59:33  lr: 0.000010  loss: 1.2747  time: 0.5466  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [2400/8855]  eta: 0:59:06  lr: 0.000010  loss: 1.5414  time: 0.5606  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [2450/8855]  eta: 0:58:39  lr: 0.000010  loss: 1.4241  time: 0.5490  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [2500/8855]  eta: 0:58:11  lr: 0.000010  loss: 1.2716  time: 0.5469  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [2550/8855]  eta: 0:57:43  lr: 0.000010  loss: 1.6237  time: 0.5489  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [2600/8855]  eta: 0:57:15  lr: 0.000010  loss: 1.1630  time: 0.5437  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [2650/8855]  eta: 0:56:48  lr: 0.000010  loss: 1.7153  time: 0.5500  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [2700/8855]  eta: 0:56:20  lr: 0.000010  loss: 1.3100  time: 0.5451  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [2750/8855]  eta: 0:55:53  lr: 0.000010  loss: 1.2042  time: 0.5458  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [2800/8855]  eta: 0:55:25  lr: 0.000010  loss: 1.2540  time: 0.5452  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [2850/8855]  eta: 0:54:57  lr: 0.000010  loss: 1.2947  time: 0.5498  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [2900/8855]  eta: 0:54:30  lr: 0.000010  loss: 1.4462  time: 0.5472  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [2950/8855]  eta: 0:54:02  lr: 0.000010  loss: 1.2541  time: 0.5474  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [3000/8855]  eta: 0:53:34  lr: 0.000010  loss: 1.5520  time: 0.5483  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [3050/8855]  eta: 0:53:07  lr: 0.000010  loss: 1.5966  time: 0.5417  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [3100/8855]  eta: 0:52:39  lr: 0.000010  loss: 1.4930  time: 0.5484  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [3150/8855]  eta: 0:52:12  lr: 0.000010  loss: 1.4443  time: 0.5521  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [3200/8855]  eta: 0:51:43  lr: 0.000010  loss: 1.3417  time: 0.5392  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [3250/8855]  eta: 0:51:15  lr: 0.000010  loss: 1.5021  time: 0.5422  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [3300/8855]  eta: 0:50:47  lr: 0.000010  loss: 1.5762  time: 0.5436  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [3350/8855]  eta: 0:50:20  lr: 0.000010  loss: 1.3785  time: 0.5403  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [3400/8855]  eta: 0:49:52  lr: 0.000010  loss: 1.3136  time: 0.5451  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [3450/8855]  eta: 0:49:25  lr: 0.000010  loss: 1.4779  time: 0.5511  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [3500/8855]  eta: 0:48:58  lr: 0.000010  loss: 1.4038  time: 0.5407  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [3550/8855]  eta: 0:48:30  lr: 0.000010  loss: 1.5908  time: 0.5456  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [3600/8855]  eta: 0:48:02  lr: 0.000010  loss: 1.4659  time: 0.5487  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [3650/8855]  eta: 0:47:35  lr: 0.000010  loss: 1.6310  time: 0.5397  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [3700/8855]  eta: 0:47:07  lr: 0.000010  loss: 1.6612  time: 0.5437  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [3750/8855]  eta: 0:46:39  lr: 0.000010  loss: 1.5099  time: 0.5418  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [3800/8855]  eta: 0:46:12  lr: 0.000010  loss: 1.5990  time: 0.5481  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [3850/8855]  eta: 0:45:44  lr: 0.000010  loss: 1.4773  time: 0.5435  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [3900/8855]  eta: 0:45:17  lr: 0.000010  loss: 1.2326  time: 0.5474  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [3950/8855]  eta: 0:44:49  lr: 0.000010  loss: 1.4755  time: 0.5484  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [4000/8855]  eta: 0:44:22  lr: 0.000010  loss: 1.5360  time: 0.5521  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [4050/8855]  eta: 0:43:54  lr: 0.000010  loss: 1.4895  time: 0.5408  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [4100/8855]  eta: 0:43:27  lr: 0.000010  loss: 1.5287  time: 0.5511  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [4150/8855]  eta: 0:43:00  lr: 0.000010  loss: 1.4686  time: 0.5483  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [4200/8855]  eta: 0:42:32  lr: 0.000010  loss: 1.3744  time: 0.5473  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [4250/8855]  eta: 0:42:05  lr: 0.000010  loss: 1.5796  time: 0.5598  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [4300/8855]  eta: 0:41:38  lr: 0.000010  loss: 1.3347  time: 0.5514  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [4350/8855]  eta: 0:41:10  lr: 0.000010  loss: 1.4717  time: 0.5505  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [4400/8855]  eta: 0:40:43  lr: 0.000010  loss: 1.5210  time: 0.5396  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [4450/8855]  eta: 0:40:16  lr: 0.000010  loss: 1.1946  time: 0.5533  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [4500/8855]  eta: 0:39:48  lr: 0.000010  loss: 1.5703  time: 0.5400  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [4550/8855]  eta: 0:39:21  lr: 0.000010  loss: 1.4027  time: 0.5511  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [4600/8855]  eta: 0:38:53  lr: 0.000010  loss: 1.3453  time: 0.5470  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [4650/8855]  eta: 0:38:26  lr: 0.000010  loss: 1.4091  time: 0.5541  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [4700/8855]  eta: 0:37:59  lr: 0.000010  loss: 1.3933  time: 0.5477  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [4750/8855]  eta: 0:37:31  lr: 0.000010  loss: 1.6088  time: 0.5438  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [4800/8855]  eta: 0:37:04  lr: 0.000010  loss: 1.6136  time: 0.5410  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [4850/8855]  eta: 0:36:36  lr: 0.000010  loss: 1.1728  time: 0.5469  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [4900/8855]  eta: 0:36:09  lr: 0.000010  loss: 1.6120  time: 0.5418  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [4950/8855]  eta: 0:35:41  lr: 0.000010  loss: 1.3131  time: 0.5464  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [5000/8855]  eta: 0:35:13  lr: 0.000010  loss: 1.2475  time: 0.5413  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [5050/8855]  eta: 0:34:46  lr: 0.000010  loss: 1.7017  time: 0.5478  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [5100/8855]  eta: 0:34:18  lr: 0.000010  loss: 1.4315  time: 0.5458  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [5150/8855]  eta: 0:33:51  lr: 0.000010  loss: 1.4632  time: 0.5515  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [5200/8855]  eta: 0:33:24  lr: 0.000010  loss: 1.4494  time: 0.5443  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [5250/8855]  eta: 0:32:56  lr: 0.000010  loss: 1.4523  time: 0.5541  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [5300/8855]  eta: 0:32:29  lr: 0.000010  loss: 1.3462  time: 0.5510  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [5350/8855]  eta: 0:32:01  lr: 0.000010  loss: 1.4394  time: 0.5464  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [5400/8855]  eta: 0:31:34  lr: 0.000010  loss: 1.1975  time: 0.5415  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [5450/8855]  eta: 0:31:06  lr: 0.000010  loss: 1.3042  time: 0.5412  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [5500/8855]  eta: 0:30:39  lr: 0.000010  loss: 1.4417  time: 0.5410  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [5550/8855]  eta: 0:30:11  lr: 0.000010  loss: 1.2311  time: 0.5456  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [5600/8855]  eta: 0:29:44  lr: 0.000010  loss: 1.2833  time: 0.5431  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [5650/8855]  eta: 0:29:17  lr: 0.000010  loss: 1.4886  time: 0.5479  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [5700/8855]  eta: 0:28:49  lr: 0.000010  loss: 1.4444  time: 0.5468  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [5750/8855]  eta: 0:28:22  lr: 0.000010  loss: 1.4975  time: 0.5533  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [5800/8855]  eta: 0:27:54  lr: 0.000010  loss: 1.3373  time: 0.5478  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [5850/8855]  eta: 0:27:27  lr: 0.000010  loss: 1.4734  time: 0.5553  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [5900/8855]  eta: 0:26:59  lr: 0.000010  loss: 1.1174  time: 0.5462  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [5950/8855]  eta: 0:26:32  lr: 0.000010  loss: 1.6718  time: 0.5480  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [6000/8855]  eta: 0:26:05  lr: 0.000010  loss: 1.3169  time: 0.5494  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [6050/8855]  eta: 0:25:37  lr: 0.000010  loss: 1.4455  time: 0.5566  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [6100/8855]  eta: 0:25:10  lr: 0.000010  loss: 1.4072  time: 0.5463  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [6150/8855]  eta: 0:24:42  lr: 0.000010  loss: 1.6028  time: 0.5478  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [6200/8855]  eta: 0:24:15  lr: 0.000010  loss: 1.4381  time: 0.5393  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [6250/8855]  eta: 0:23:47  lr: 0.000010  loss: 1.3514  time: 0.5472  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [6300/8855]  eta: 0:23:20  lr: 0.000010  loss: 1.4374  time: 0.5548  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [6350/8855]  eta: 0:22:53  lr: 0.000010  loss: 1.5123  time: 0.5453  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [6400/8855]  eta: 0:22:25  lr: 0.000010  loss: 1.6372  time: 0.5442  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [6450/8855]  eta: 0:21:58  lr: 0.000010  loss: 1.3697  time: 0.5550  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [6500/8855]  eta: 0:21:31  lr: 0.000010  loss: 1.3873  time: 0.5523  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [6550/8855]  eta: 0:21:03  lr: 0.000010  loss: 1.7995  time: 0.5551  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [6600/8855]  eta: 0:20:36  lr: 0.000010  loss: 1.4551  time: 0.5512  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [6650/8855]  eta: 0:20:08  lr: 0.000010  loss: 1.5635  time: 0.5405  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [6700/8855]  eta: 0:19:41  lr: 0.000010  loss: 1.4455  time: 0.5462  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [6750/8855]  eta: 0:19:13  lr: 0.000010  loss: 1.4979  time: 0.5520  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [6800/8855]  eta: 0:18:46  lr: 0.000010  loss: 1.4859  time: 0.5436  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [6850/8855]  eta: 0:18:19  lr: 0.000010  loss: 1.2038  time: 0.5469  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [6900/8855]  eta: 0:17:51  lr: 0.000010  loss: 1.2193  time: 0.5432  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [6950/8855]  eta: 0:17:24  lr: 0.000010  loss: 1.3802  time: 0.5546  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [7000/8855]  eta: 0:16:56  lr: 0.000010  loss: 1.3296  time: 0.5410  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [7050/8855]  eta: 0:16:29  lr: 0.000010  loss: 1.4462  time: 0.5495  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [7100/8855]  eta: 0:16:02  lr: 0.000010  loss: 1.5180  time: 0.5462  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [7150/8855]  eta: 0:15:34  lr: 0.000010  loss: 1.5987  time: 0.5380  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [7200/8855]  eta: 0:15:07  lr: 0.000010  loss: 1.3175  time: 0.5474  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [7250/8855]  eta: 0:14:39  lr: 0.000010  loss: 1.4249  time: 0.5470  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [7300/8855]  eta: 0:14:12  lr: 0.000010  loss: 1.3347  time: 0.5449  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [7350/8855]  eta: 0:13:44  lr: 0.000010  loss: 1.4823  time: 0.5445  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [7400/8855]  eta: 0:13:17  lr: 0.000010  loss: 1.4069  time: 0.5451  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [7450/8855]  eta: 0:12:50  lr: 0.000010  loss: 1.4937  time: 0.5448  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [7500/8855]  eta: 0:12:22  lr: 0.000010  loss: 1.5281  time: 0.5467  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [7550/8855]  eta: 0:11:55  lr: 0.000010  loss: 1.5622  time: 0.5442  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [7600/8855]  eta: 0:11:27  lr: 0.000010  loss: 1.4998  time: 0.5500  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [7650/8855]  eta: 0:11:00  lr: 0.000010  loss: 1.4890  time: 0.5421  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [7700/8855]  eta: 0:10:33  lr: 0.000010  loss: 1.5134  time: 0.5449  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [7750/8855]  eta: 0:10:05  lr: 0.000010  loss: 1.2283  time: 0.5509  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [7800/8855]  eta: 0:09:38  lr: 0.000010  loss: 1.7045  time: 0.5387  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [7850/8855]  eta: 0:09:10  lr: 0.000010  loss: 1.3757  time: 0.5433  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [7900/8855]  eta: 0:08:43  lr: 0.000010  loss: 1.3800  time: 0.5431  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [7950/8855]  eta: 0:08:15  lr: 0.000010  loss: 1.5070  time: 0.5446  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [8000/8855]  eta: 0:07:48  lr: 0.000010  loss: 1.3983  time: 0.5486  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [8050/8855]  eta: 0:07:21  lr: 0.000010  loss: 1.4719  time: 0.5475  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [8100/8855]  eta: 0:06:53  lr: 0.000010  loss: 1.5155  time: 0.5421  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [8150/8855]  eta: 0:06:26  lr: 0.000010  loss: 1.1121  time: 0.5500  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [8200/8855]  eta: 0:05:58  lr: 0.000010  loss: 1.3887  time: 0.5594  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [8250/8855]  eta: 0:05:31  lr: 0.000010  loss: 1.2155  time: 0.5494  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [8300/8855]  eta: 0:05:04  lr: 0.000010  loss: 1.2304  time: 0.5537  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [8350/8855]  eta: 0:04:36  lr: 0.000010  loss: 1.6078  time: 0.5457  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [8400/8855]  eta: 0:04:09  lr: 0.000010  loss: 1.4440  time: 0.5504  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [8450/8855]  eta: 0:03:41  lr: 0.000010  loss: 1.4137  time: 0.5410  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [8500/8855]  eta: 0:03:14  lr: 0.000010  loss: 1.5637  time: 0.5573  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [8550/8855]  eta: 0:02:47  lr: 0.000010  loss: 1.4755  time: 0.5457  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [8600/8855]  eta: 0:02:19  lr: 0.000010  loss: 1.3587  time: 0.5482  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [8650/8855]  eta: 0:01:52  lr: 0.000010  loss: 1.5463  time: 0.5554  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [8700/8855]  eta: 0:01:24  lr: 0.000010  loss: 1.5185  time: 0.5492  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [8750/8855]  eta: 0:00:57  lr: 0.000010  loss: 1.3881  time: 0.5455  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [8800/8855]  eta: 0:00:30  lr: 0.000010  loss: 1.2508  time: 0.5508  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [8850/8855]  eta: 0:00:02  lr: 0.000010  loss: 1.2455  time: 0.5430  data: 0.0000  max mem: 32954
Train: data epoch: [0]  [8854/8855]  eta: 0:00:00  lr: 0.000010  loss: 1.1960  time: 0.5591  data: 0.0000  max mem: 32954
Train: data epoch: [0] Total time: 1:20:53 (0.5481 s / it)
2023-05-30 21:27:09,308 [INFO] Averaged stats: lr: 0.0000  loss: 1.4510
2023-05-30 21:27:09,311 [INFO] Evaluating on val.
Evaluation  [  0/157]  eta: 0:06:07    time: 2.3433  data: 0.8686  max mem: 32954
Evaluation  [ 10/157]  eta: 0:03:39    time: 1.4903  data: 0.0808  max mem: 32954
Evaluation  [ 20/157]  eta: 0:03:15    time: 1.3849  data: 0.0018  max mem: 32954
Evaluation  [ 30/157]  eta: 0:02:59    time: 1.3720  data: 0.0014  max mem: 32954
Evaluation  [ 40/157]  eta: 0:02:44    time: 1.3753  data: 0.0014  max mem: 32954
Evaluation  [ 50/157]  eta: 0:02:29    time: 1.3647  data: 0.0014  max mem: 32954
Evaluation  [ 60/157]  eta: 0:02:14    time: 1.3595  data: 0.0014  max mem: 32954
Evaluation  [ 70/157]  eta: 0:02:01    time: 1.3946  data: 0.0014  max mem: 32954
Evaluation  [ 80/157]  eta: 0:01:47    time: 1.4049  data: 0.0014  max mem: 32954
Evaluation  [ 90/157]  eta: 0:01:33    time: 1.3891  data: 0.0014  max mem: 32954
Evaluation  [100/157]  eta: 0:01:19    time: 1.3866  data: 0.0014  max mem: 32954
Evaluation  [110/157]  eta: 0:01:05    time: 1.3546  data: 0.0014  max mem: 32954
Evaluation  [120/157]  eta: 0:00:51    time: 1.3665  data: 0.0014  max mem: 32954
Evaluation  [130/157]  eta: 0:00:37    time: 1.3911  data: 0.0014  max mem: 32954
Evaluation  [140/157]  eta: 0:00:23    time: 1.3602  data: 0.0014  max mem: 32954
Evaluation  [150/157]  eta: 0:00:09    time: 1.3508  data: 0.0014  max mem: 32954
Evaluation  [156/157]  eta: 0:00:01    time: 1.3323  data: 0.0115  max mem: 32954
Evaluation Total time: 0:03:36 (1.3790 s / it)
2023-05-30 21:30:45,836 [WARNING] rank 0 starts merging results.
result file saved to /root/Documents/DEMOS/InstructBLIP/output/BLIP2_Instruct/Caption_coco/20230530200/result/val_epoch0.json
Using downloaded and verified file: /root/Documents/CACHE/lavis/coco_gt/coco_karpathy_val_gt.json
loading annotations into memory...
Done (t=0.03s)
creating index...
index created!
Loading and preparing results...
DONE (t=0.01s)
creating index...
index created!
tokenization...
PTBTokenizer tokenized 307821 tokens at 1249969.87 tokens per second.
PTBTokenizer tokenized 70365 tokens at 490391.85 tokens per second.
setting up scorers...
computing Bleu score...
{'testlen': 64942, 'reflen': 59189, 'guess': [64942, 59942, 54942, 49942], 'correct': [46913, 27151, 14470, 7470]}
ratio: 1.0971971143286574
Bleu_1: 0.722
Bleu_2: 0.572
Bleu_3: 0.442
Bleu_4: 0.337
computing METEOR score...
METEOR: 0.316
computing Rouge score...
ROUGE_L: 0.566
computing CIDEr score...
CIDEr: 1.198
computing SPICE score...
WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.nustaq.serialization.FSTClazzInfo (file:/data/root/anaconda3/envs/lavis/lib/python3.8/site-packages/pycocoevalcap/spice/lib/fst-2.47.jar) to field java.lang.String.value
WARNING: Please consider reporting this to the maintainers of org.nustaq.serialization.FSTClazzInfo
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
Parsing reference captions
Parsing test captions
Initiating Stanford parsing pipeline
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize
[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse
[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... 
done [0.4 sec].
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner
Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.0 sec].
Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.4 sec].
Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.5 sec].
Threads( StanfordCoreNLP ) [11.566 seconds]
Warning: Nashorn engine is planned to be removed from a future JDK release
SPICE evaluation took: 20.45 s
SPICE: 0.256
Bleu_1: 0.722
Bleu_2: 0.572
Bleu_3: 0.442
Bleu_4: 0.337
METEOR: 0.316
ROUGE_L: 0.566
CIDEr: 1.198
SPICE: 0.256
2023-05-30 21:31:30,094 [INFO] Saving checkpoint at epoch 0 to /root/Documents/DEMOS/InstructBLIP/output/BLIP2_Instruct/Caption_coco/20230530200/checkpoint_best.pth.
2023-05-30 21:31:32,272 [INFO] Start training
2023-05-30 21:31:32,300 [INFO] Start training epoch 1, 8855 iters per inner epoch.
/root/anaconda3/envs/lavis/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
Train: data epoch: [1]  [   0/8855]  eta: 8:52:27  lr: 0.000009  loss: 1.4695  time: 3.6078  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [  50/8855]  eta: 1:29:04  lr: 0.000009  loss: 1.3773  time: 0.5474  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [ 100/8855]  eta: 1:24:09  lr: 0.000009  loss: 1.3512  time: 0.5454  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [ 150/8855]  eta: 1:22:23  lr: 0.000009  loss: 1.4886  time: 0.5506  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [ 200/8855]  eta: 1:21:06  lr: 0.000009  loss: 1.3602  time: 0.5429  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [ 250/8855]  eta: 1:20:08  lr: 0.000009  loss: 1.4067  time: 0.5506  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [ 300/8855]  eta: 1:19:28  lr: 0.000009  loss: 1.1464  time: 0.5577  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [ 350/8855]  eta: 1:18:52  lr: 0.000009  loss: 1.4158  time: 0.5494  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [ 400/8855]  eta: 1:18:20  lr: 0.000009  loss: 1.5222  time: 0.5477  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [ 450/8855]  eta: 1:17:48  lr: 0.000009  loss: 1.5540  time: 0.5521  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [ 500/8855]  eta: 1:17:14  lr: 0.000009  loss: 1.3444  time: 0.5502  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [ 550/8855]  eta: 1:16:41  lr: 0.000009  loss: 1.5424  time: 0.5433  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [ 600/8855]  eta: 1:16:09  lr: 0.000009  loss: 1.7352  time: 0.5435  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [ 650/8855]  eta: 1:15:37  lr: 0.000009  loss: 1.6985  time: 0.5456  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [ 700/8855]  eta: 1:15:05  lr: 0.000009  loss: 1.6614  time: 0.5506  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [ 750/8855]  eta: 1:14:34  lr: 0.000009  loss: 1.3278  time: 0.5445  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [ 800/8855]  eta: 1:14:03  lr: 0.000009  loss: 1.3535  time: 0.5431  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [ 850/8855]  eta: 1:13:33  lr: 0.000009  loss: 1.5156  time: 0.5424  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [ 900/8855]  eta: 1:13:03  lr: 0.000009  loss: 1.3888  time: 0.5540  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [ 950/8855]  eta: 1:12:35  lr: 0.000009  loss: 1.6111  time: 0.5518  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [1000/8855]  eta: 1:12:07  lr: 0.000009  loss: 1.4275  time: 0.5428  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [1050/8855]  eta: 1:11:39  lr: 0.000009  loss: 1.2756  time: 0.5406  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [1100/8855]  eta: 1:11:10  lr: 0.000009  loss: 1.3626  time: 0.5484  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [1150/8855]  eta: 1:10:41  lr: 0.000009  loss: 1.2773  time: 0.5452  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [1200/8855]  eta: 1:10:12  lr: 0.000009  loss: 1.4093  time: 0.5488  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [1250/8855]  eta: 1:09:42  lr: 0.000009  loss: 1.2853  time: 0.5457  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [1300/8855]  eta: 1:09:13  lr: 0.000009  loss: 1.4405  time: 0.5445  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [1350/8855]  eta: 1:08:44  lr: 0.000009  loss: 1.4475  time: 0.5427  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [1400/8855]  eta: 1:08:15  lr: 0.000009  loss: 1.4906  time: 0.5424  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [1450/8855]  eta: 1:07:48  lr: 0.000009  loss: 1.2086  time: 0.5558  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [1500/8855]  eta: 1:07:20  lr: 0.000009  loss: 1.3267  time: 0.5441  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [1550/8855]  eta: 1:06:52  lr: 0.000009  loss: 1.3423  time: 0.5441  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [1600/8855]  eta: 1:06:25  lr: 0.000009  loss: 1.5704  time: 0.5587  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [1650/8855]  eta: 1:05:57  lr: 0.000009  loss: 1.5248  time: 0.5430  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [1700/8855]  eta: 1:05:31  lr: 0.000009  loss: 1.5394  time: 0.5596  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [1750/8855]  eta: 1:05:03  lr: 0.000009  loss: 1.3234  time: 0.5402  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [1800/8855]  eta: 1:04:35  lr: 0.000009  loss: 1.6068  time: 0.5506  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [1850/8855]  eta: 1:04:07  lr: 0.000009  loss: 1.4407  time: 0.5513  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [1900/8855]  eta: 1:03:41  lr: 0.000009  loss: 1.3770  time: 0.5628  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [1950/8855]  eta: 1:03:13  lr: 0.000009  loss: 1.2925  time: 0.5531  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [2000/8855]  eta: 1:02:45  lr: 0.000009  loss: 1.3310  time: 0.5458  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [2050/8855]  eta: 1:02:17  lr: 0.000009  loss: 1.5294  time: 0.5466  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [2100/8855]  eta: 1:01:49  lr: 0.000009  loss: 1.4881  time: 0.5448  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [2150/8855]  eta: 1:01:21  lr: 0.000009  loss: 1.2473  time: 0.5528  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [2200/8855]  eta: 1:00:54  lr: 0.000009  loss: 1.2589  time: 0.5537  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [2250/8855]  eta: 1:00:26  lr: 0.000009  loss: 1.3631  time: 0.5412  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [2300/8855]  eta: 0:59:58  lr: 0.000009  loss: 1.4283  time: 0.5519  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [2350/8855]  eta: 0:59:31  lr: 0.000009  loss: 1.4760  time: 0.5518  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [2400/8855]  eta: 0:59:04  lr: 0.000009  loss: 1.4094  time: 0.5493  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [2450/8855]  eta: 0:58:36  lr: 0.000009  loss: 1.5305  time: 0.5459  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [2500/8855]  eta: 0:58:08  lr: 0.000009  loss: 1.4533  time: 0.5466  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [2550/8855]  eta: 0:57:40  lr: 0.000009  loss: 1.4066  time: 0.5394  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [2600/8855]  eta: 0:57:12  lr: 0.000009  loss: 1.5661  time: 0.5493  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [2650/8855]  eta: 0:56:45  lr: 0.000009  loss: 1.3190  time: 0.5506  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [2700/8855]  eta: 0:56:17  lr: 0.000009  loss: 1.4978  time: 0.5409  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [2750/8855]  eta: 0:55:49  lr: 0.000009  loss: 1.6673  time: 0.5464  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [2800/8855]  eta: 0:55:21  lr: 0.000009  loss: 1.5620  time: 0.5519  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [2850/8855]  eta: 0:54:54  lr: 0.000009  loss: 1.6227  time: 0.5462  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [2900/8855]  eta: 0:54:27  lr: 0.000009  loss: 1.3262  time: 0.5452  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [2950/8855]  eta: 0:53:59  lr: 0.000009  loss: 1.3516  time: 0.5493  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [3000/8855]  eta: 0:53:32  lr: 0.000009  loss: 1.3240  time: 0.5459  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [3050/8855]  eta: 0:53:04  lr: 0.000009  loss: 1.5499  time: 0.5468  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [3100/8855]  eta: 0:52:37  lr: 0.000009  loss: 1.4099  time: 0.5543  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [3150/8855]  eta: 0:52:09  lr: 0.000009  loss: 1.5396  time: 0.5426  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [3200/8855]  eta: 0:51:42  lr: 0.000009  loss: 1.5133  time: 0.5525  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [3250/8855]  eta: 0:51:14  lr: 0.000009  loss: 1.5423  time: 0.5503  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [3300/8855]  eta: 0:50:47  lr: 0.000009  loss: 1.4547  time: 0.5417  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [3350/8855]  eta: 0:50:19  lr: 0.000009  loss: 1.6190  time: 0.5429  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [3400/8855]  eta: 0:49:52  lr: 0.000009  loss: 1.4472  time: 0.5459  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [3450/8855]  eta: 0:49:24  lr: 0.000009  loss: 1.2464  time: 0.5406  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [3500/8855]  eta: 0:48:57  lr: 0.000009  loss: 1.3975  time: 0.5447  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [3550/8855]  eta: 0:48:29  lr: 0.000009  loss: 1.5898  time: 0.5436  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [3600/8855]  eta: 0:48:01  lr: 0.000009  loss: 1.5336  time: 0.5436  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [3650/8855]  eta: 0:47:34  lr: 0.000009  loss: 1.3765  time: 0.5502  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [3700/8855]  eta: 0:47:07  lr: 0.000009  loss: 1.7740  time: 0.5504  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [3750/8855]  eta: 0:46:39  lr: 0.000009  loss: 1.4296  time: 0.5443  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [3800/8855]  eta: 0:46:11  lr: 0.000009  loss: 1.4324  time: 0.5493  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [3850/8855]  eta: 0:45:44  lr: 0.000009  loss: 1.5212  time: 0.5508  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [3900/8855]  eta: 0:45:17  lr: 0.000009  loss: 1.5397  time: 0.5504  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [3950/8855]  eta: 0:44:49  lr: 0.000009  loss: 1.3416  time: 0.5507  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [4000/8855]  eta: 0:44:22  lr: 0.000009  loss: 1.2292  time: 0.5419  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [4050/8855]  eta: 0:43:54  lr: 0.000009  loss: 1.6073  time: 0.5528  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [4100/8855]  eta: 0:43:27  lr: 0.000009  loss: 1.7170  time: 0.5440  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [4150/8855]  eta: 0:42:59  lr: 0.000009  loss: 1.3806  time: 0.5463  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [4200/8855]  eta: 0:42:32  lr: 0.000009  loss: 1.2692  time: 0.5479  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [4250/8855]  eta: 0:42:05  lr: 0.000009  loss: 1.4926  time: 0.5493  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [4300/8855]  eta: 0:41:37  lr: 0.000009  loss: 1.5151  time: 0.5426  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [4350/8855]  eta: 0:41:09  lr: 0.000009  loss: 1.2479  time: 0.5422  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [4400/8855]  eta: 0:40:42  lr: 0.000009  loss: 1.5591  time: 0.5438  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [4450/8855]  eta: 0:40:14  lr: 0.000009  loss: 1.4212  time: 0.5520  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [4500/8855]  eta: 0:39:47  lr: 0.000009  loss: 1.3550  time: 0.5464  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [4550/8855]  eta: 0:39:19  lr: 0.000009  loss: 1.5759  time: 0.5485  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [4600/8855]  eta: 0:38:52  lr: 0.000009  loss: 1.4385  time: 0.5459  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [4650/8855]  eta: 0:38:25  lr: 0.000009  loss: 1.4556  time: 0.5510  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [4700/8855]  eta: 0:37:57  lr: 0.000009  loss: 1.3425  time: 0.5466  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [4750/8855]  eta: 0:37:30  lr: 0.000009  loss: 1.4052  time: 0.5442  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [4800/8855]  eta: 0:37:02  lr: 0.000009  loss: 1.5968  time: 0.5483  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [4850/8855]  eta: 0:36:35  lr: 0.000009  loss: 1.6380  time: 0.5526  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [4900/8855]  eta: 0:36:08  lr: 0.000009  loss: 1.5242  time: 0.5514  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [4950/8855]  eta: 0:35:40  lr: 0.000009  loss: 1.5199  time: 0.5442  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [5000/8855]  eta: 0:35:13  lr: 0.000009  loss: 1.4079  time: 0.5450  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [5050/8855]  eta: 0:34:45  lr: 0.000009  loss: 1.3861  time: 0.5546  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [5100/8855]  eta: 0:34:18  lr: 0.000009  loss: 1.2205  time: 0.5468  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [5150/8855]  eta: 0:33:50  lr: 0.000009  loss: 1.6279  time: 0.5584  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [5200/8855]  eta: 0:33:23  lr: 0.000009  loss: 1.3549  time: 0.5462  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [5250/8855]  eta: 0:32:56  lr: 0.000009  loss: 1.3011  time: 0.5546  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [5300/8855]  eta: 0:32:28  lr: 0.000009  loss: 1.1206  time: 0.5492  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [5350/8855]  eta: 0:32:01  lr: 0.000009  loss: 1.3948  time: 0.5557  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [5400/8855]  eta: 0:31:33  lr: 0.000009  loss: 1.2225  time: 0.5431  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [5450/8855]  eta: 0:31:06  lr: 0.000009  loss: 1.2075  time: 0.5494  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [5500/8855]  eta: 0:30:38  lr: 0.000009  loss: 1.3788  time: 0.5483  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [5550/8855]  eta: 0:30:11  lr: 0.000009  loss: 1.3353  time: 0.5474  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [5600/8855]  eta: 0:29:44  lr: 0.000009  loss: 1.4200  time: 0.5415  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [5650/8855]  eta: 0:29:16  lr: 0.000009  loss: 1.3117  time: 0.5478  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [5700/8855]  eta: 0:28:49  lr: 0.000009  loss: 1.4029  time: 0.5442  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [5750/8855]  eta: 0:28:21  lr: 0.000009  loss: 1.5066  time: 0.5540  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [5800/8855]  eta: 0:27:54  lr: 0.000009  loss: 1.3668  time: 0.5377  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [5850/8855]  eta: 0:27:26  lr: 0.000009  loss: 1.3868  time: 0.5540  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [5900/8855]  eta: 0:26:59  lr: 0.000009  loss: 1.3732  time: 0.5446  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [5950/8855]  eta: 0:26:31  lr: 0.000009  loss: 1.6245  time: 0.5407  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [6000/8855]  eta: 0:26:04  lr: 0.000009  loss: 1.4995  time: 0.5455  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [6050/8855]  eta: 0:25:36  lr: 0.000009  loss: 1.2651  time: 0.5437  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [6100/8855]  eta: 0:25:09  lr: 0.000009  loss: 1.3793  time: 0.5449  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [6150/8855]  eta: 0:24:42  lr: 0.000009  loss: 1.5994  time: 0.5448  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [6200/8855]  eta: 0:24:14  lr: 0.000009  loss: 1.3563  time: 0.5511  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [6250/8855]  eta: 0:23:47  lr: 0.000009  loss: 1.3724  time: 0.5434  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [6300/8855]  eta: 0:23:19  lr: 0.000009  loss: 1.3609  time: 0.5519  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [6350/8855]  eta: 0:22:52  lr: 0.000009  loss: 1.1915  time: 0.5527  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [6400/8855]  eta: 0:22:25  lr: 0.000009  loss: 1.4253  time: 0.5464  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [6450/8855]  eta: 0:21:57  lr: 0.000009  loss: 1.6157  time: 0.5390  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [6500/8855]  eta: 0:21:30  lr: 0.000009  loss: 1.2498  time: 0.5490  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [6550/8855]  eta: 0:21:02  lr: 0.000009  loss: 1.4174  time: 0.5404  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [6600/8855]  eta: 0:20:35  lr: 0.000009  loss: 1.4229  time: 0.5493  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [6650/8855]  eta: 0:20:07  lr: 0.000009  loss: 1.2954  time: 0.5429  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [6700/8855]  eta: 0:19:40  lr: 0.000009  loss: 1.3957  time: 0.5449  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [6750/8855]  eta: 0:19:13  lr: 0.000009  loss: 1.6895  time: 0.5514  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [6800/8855]  eta: 0:18:45  lr: 0.000009  loss: 1.5660  time: 0.5442  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [6850/8855]  eta: 0:18:18  lr: 0.000009  loss: 1.3746  time: 0.5515  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [6900/8855]  eta: 0:17:50  lr: 0.000009  loss: 1.7938  time: 0.5453  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [6950/8855]  eta: 0:17:23  lr: 0.000009  loss: 1.4652  time: 0.5467  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [7000/8855]  eta: 0:16:56  lr: 0.000009  loss: 1.3456  time: 0.5473  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [7050/8855]  eta: 0:16:28  lr: 0.000009  loss: 1.4027  time: 0.5548  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [7100/8855]  eta: 0:16:01  lr: 0.000009  loss: 1.5427  time: 0.5487  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [7150/8855]  eta: 0:15:34  lr: 0.000009  loss: 1.2684  time: 0.5420  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [7200/8855]  eta: 0:15:06  lr: 0.000009  loss: 1.3160  time: 0.5495  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [7250/8855]  eta: 0:14:39  lr: 0.000009  loss: 1.5312  time: 0.5523  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [7300/8855]  eta: 0:14:11  lr: 0.000009  loss: 1.4840  time: 0.5489  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [7350/8855]  eta: 0:13:44  lr: 0.000009  loss: 1.3165  time: 0.5501  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [7400/8855]  eta: 0:13:17  lr: 0.000009  loss: 1.6283  time: 0.5437  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [7450/8855]  eta: 0:12:49  lr: 0.000009  loss: 1.2359  time: 0.5394  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [7500/8855]  eta: 0:12:22  lr: 0.000009  loss: 1.7437  time: 0.5475  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [7550/8855]  eta: 0:11:54  lr: 0.000009  loss: 1.3836  time: 0.5529  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [7600/8855]  eta: 0:11:27  lr: 0.000009  loss: 1.5247  time: 0.5420  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [7650/8855]  eta: 0:11:00  lr: 0.000009  loss: 1.1984  time: 0.5544  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [7700/8855]  eta: 0:10:32  lr: 0.000009  loss: 1.3052  time: 0.5594  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [7750/8855]  eta: 0:10:05  lr: 0.000009  loss: 1.2278  time: 0.5485  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [7800/8855]  eta: 0:09:37  lr: 0.000009  loss: 1.1566  time: 0.5519  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [7850/8855]  eta: 0:09:10  lr: 0.000009  loss: 1.3251  time: 0.5482  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [7900/8855]  eta: 0:08:43  lr: 0.000009  loss: 1.5740  time: 0.5441  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [7950/8855]  eta: 0:08:15  lr: 0.000009  loss: 1.3466  time: 0.5409  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [8000/8855]  eta: 0:07:48  lr: 0.000009  loss: 1.3392  time: 0.5494  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [8050/8855]  eta: 0:07:20  lr: 0.000009  loss: 1.4890  time: 0.5456  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [8100/8855]  eta: 0:06:53  lr: 0.000009  loss: 1.4763  time: 0.5446  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [8150/8855]  eta: 0:06:26  lr: 0.000009  loss: 1.4532  time: 0.5504  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [8200/8855]  eta: 0:05:58  lr: 0.000009  loss: 1.6324  time: 0.5517  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [8250/8855]  eta: 0:05:31  lr: 0.000009  loss: 1.4333  time: 0.5435  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [8300/8855]  eta: 0:05:04  lr: 0.000009  loss: 1.4511  time: 0.5553  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [8350/8855]  eta: 0:04:36  lr: 0.000009  loss: 1.5211  time: 0.5491  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [8400/8855]  eta: 0:04:09  lr: 0.000009  loss: 1.4195  time: 0.5496  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [8450/8855]  eta: 0:03:41  lr: 0.000009  loss: 1.2958  time: 0.5447  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [8500/8855]  eta: 0:03:14  lr: 0.000009  loss: 1.4042  time: 0.5493  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [8550/8855]  eta: 0:02:47  lr: 0.000009  loss: 1.3816  time: 0.5428  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [8600/8855]  eta: 0:02:19  lr: 0.000009  loss: 1.3968  time: 0.5486  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [8650/8855]  eta: 0:01:52  lr: 0.000009  loss: 1.3589  time: 0.5459  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [8700/8855]  eta: 0:01:24  lr: 0.000009  loss: 1.4852  time: 0.5559  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [8750/8855]  eta: 0:00:57  lr: 0.000009  loss: 1.3951  time: 0.5514  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [8800/8855]  eta: 0:00:30  lr: 0.000009  loss: 1.4890  time: 0.5575  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [8850/8855]  eta: 0:00:02  lr: 0.000009  loss: 1.5851  time: 0.5474  data: 0.0000  max mem: 32954
Train: data epoch: [1]  [8854/8855]  eta: 0:00:00  lr: 0.000009  loss: 1.2524  time: 0.5563  data: 0.0000  max mem: 32954
Train: data epoch: [1] Total time: 1:20:51 (0.5478 s / it)
2023-05-30 22:52:23,509 [INFO] Averaged stats: lr: 0.0000  loss: 1.4276
2023-05-30 22:52:23,512 [INFO] Evaluating on val.
Evaluation  [  0/157]  eta: 0:06:21    time: 2.4283  data: 0.8415  max mem: 32954
Evaluation  [ 10/157]  eta: 0:03:40    time: 1.4998  data: 0.0778  max mem: 32954
Evaluation  [ 20/157]  eta: 0:03:18    time: 1.3966  data: 0.0014  max mem: 32954
Evaluation  [ 30/157]  eta: 0:03:02    time: 1.3985  data: 0.0014  max mem: 32954
Evaluation  [ 40/157]  eta: 0:02:45    time: 1.3841  data: 0.0014  max mem: 32954
Evaluation  [ 50/157]  eta: 0:02:30    time: 1.3621  data: 0.0014  max mem: 32954
Evaluation  [ 60/157]  eta: 0:02:15    time: 1.3571  data: 0.0014  max mem: 32954
Evaluation  [ 70/157]  eta: 0:02:01    time: 1.3581  data: 0.0014  max mem: 32954
Evaluation  [ 80/157]  eta: 0:01:47    time: 1.3753  data: 0.0014  max mem: 32954
Evaluation  [ 90/157]  eta: 0:01:33    time: 1.3867  data: 0.0014  max mem: 32954
Evaluation  [100/157]  eta: 0:01:19    time: 1.3825  data: 0.0014  max mem: 32954
Evaluation  [110/157]  eta: 0:01:05    time: 1.3586  data: 0.0014  max mem: 32954
Evaluation  [120/157]  eta: 0:00:51    time: 1.3522  data: 0.0014  max mem: 32954
Evaluation  [130/157]  eta: 0:00:37    time: 1.3721  data: 0.0014  max mem: 32954
Evaluation  [140/157]  eta: 0:00:23    time: 1.3457  data: 0.0014  max mem: 32954
Evaluation  [150/157]  eta: 0:00:09    time: 1.3079  data: 0.0014  max mem: 32954
Evaluation  [156/157]  eta: 0:00:01    time: 1.3006  data: 0.0094  max mem: 32954
Evaluation Total time: 0:03:35 (1.3697 s / it)
2023-05-30 22:55:58,573 [WARNING] rank 0 starts merging results.
result file saved to /root/Documents/DEMOS/InstructBLIP/output/BLIP2_Instruct/Caption_coco/20230530200/result/val_epoch1.json
Using downloaded and verified file: /root/Documents/CACHE/lavis/coco_gt/coco_karpathy_val_gt.json
loading annotations into memory...
Done (t=0.02s)
creating index...
index created!
Loading and preparing results...
DONE (t=0.01s)
creating index...
index created!
tokenization...
PTBTokenizer tokenized 307821 tokens at 1234318.52 tokens per second.
PTBTokenizer tokenized 69717 tokens at 485710.66 tokens per second.
setting up scorers...
computing Bleu score...
{'testlen': 64307, 'reflen': 58837, 'guess': [64307, 59307, 54307, 49307], 'correct': [46945, 26941, 14194, 7269]}
ratio: 1.0929687101653536
Bleu_1: 0.730
Bleu_2: 0.576
Bleu_3: 0.443
Bleu_4: 0.336
computing METEOR score...
METEOR: 0.316
computing Rouge score...
ROUGE_L: 0.567
computing CIDEr score...
CIDEr: 1.208
computing SPICE score...
WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.nustaq.serialization.FSTClazzInfo (file:/data/root/anaconda3/envs/lavis/lib/python3.8/site-packages/pycocoevalcap/spice/lib/fst-2.47.jar) to field java.lang.String.value
WARNING: Please consider reporting this to the maintainers of org.nustaq.serialization.FSTClazzInfo
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
Parsing reference captions
Parsing test captions
Initiating Stanford parsing pipeline
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize
[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse
[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... 
done [0.4 sec].
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner
Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [0.9 sec].
Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.4 sec].
Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.6 sec].
Threads( StanfordCoreNLP ) [8.273 seconds]
Warning: Nashorn engine is planned to be removed from a future JDK release
SPICE evaluation took: 17.38 s
SPICE: 0.255
Bleu_1: 0.730
Bleu_2: 0.576
Bleu_3: 0.443
Bleu_4: 0.336
METEOR: 0.316
ROUGE_L: 0.567
CIDEr: 1.208
SPICE: 0.255
2023-05-30 22:56:40,893 [INFO] Saving checkpoint at epoch 1 to /root/Documents/DEMOS/InstructBLIP/output/BLIP2_Instruct/Caption_coco/20230530200/checkpoint_best.pth.
2023-05-30 22:56:44,377 [INFO] Start training
2023-05-30 22:56:44,443 [INFO] Start training epoch 2, 8855 iters per inner epoch.
/root/anaconda3/envs/lavis/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
Train: data epoch: [2]  [   0/8855]  eta: 8:49:54  lr: 0.000007  loss: 1.1909  time: 3.5906  data: 0.0001  max mem: 32954
Train: data epoch: [2]  [  50/8855]  eta: 1:28:48  lr: 0.000007  loss: 1.4396  time: 0.5414  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [ 100/8855]  eta: 1:24:02  lr: 0.000007  loss: 1.4118  time: 0.5425  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [ 150/8855]  eta: 1:22:12  lr: 0.000007  loss: 1.3809  time: 0.5531  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [ 200/8855]  eta: 1:21:00  lr: 0.000007  loss: 1.5672  time: 0.5445  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [ 250/8855]  eta: 1:20:21  lr: 0.000007  loss: 1.4786  time: 0.5615  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [ 300/8855]  eta: 1:19:33  lr: 0.000007  loss: 1.5437  time: 0.5462  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [ 350/8855]  eta: 1:18:51  lr: 0.000007  loss: 1.5413  time: 0.5457  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [ 400/8855]  eta: 1:18:12  lr: 0.000007  loss: 1.3737  time: 0.5453  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [ 450/8855]  eta: 1:17:39  lr: 0.000007  loss: 1.4871  time: 0.5460  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [ 500/8855]  eta: 1:17:05  lr: 0.000007  loss: 1.6618  time: 0.5481  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [ 550/8855]  eta: 1:16:36  lr: 0.000007  loss: 1.5914  time: 0.5426  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [ 600/8855]  eta: 1:16:04  lr: 0.000007  loss: 1.2412  time: 0.5427  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [ 650/8855]  eta: 1:15:35  lr: 0.000007  loss: 1.2545  time: 0.5426  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [ 700/8855]  eta: 1:15:05  lr: 0.000007  loss: 1.3370  time: 0.5421  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [ 750/8855]  eta: 1:14:32  lr: 0.000007  loss: 1.4365  time: 0.5402  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [ 800/8855]  eta: 1:14:00  lr: 0.000007  loss: 1.3326  time: 0.5400  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [ 850/8855]  eta: 1:13:32  lr: 0.000007  loss: 1.2954  time: 0.5450  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [ 900/8855]  eta: 1:13:05  lr: 0.000007  loss: 1.3238  time: 0.5535  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [ 950/8855]  eta: 1:12:37  lr: 0.000007  loss: 1.4477  time: 0.5583  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [1000/8855]  eta: 1:12:08  lr: 0.000007  loss: 1.4277  time: 0.5469  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [1050/8855]  eta: 1:11:40  lr: 0.000007  loss: 1.1559  time: 0.5482  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [1100/8855]  eta: 1:11:14  lr: 0.000007  loss: 1.1964  time: 0.5672  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [1150/8855]  eta: 1:10:45  lr: 0.000007  loss: 1.6239  time: 0.5441  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [1200/8855]  eta: 1:10:16  lr: 0.000007  loss: 1.5169  time: 0.5460  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [1250/8855]  eta: 1:09:49  lr: 0.000007  loss: 1.6653  time: 0.5460  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [1300/8855]  eta: 1:09:21  lr: 0.000007  loss: 1.3141  time: 0.5432  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [1350/8855]  eta: 1:08:53  lr: 0.000007  loss: 1.2417  time: 0.5466  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [1400/8855]  eta: 1:08:24  lr: 0.000007  loss: 1.4190  time: 0.5449  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [1450/8855]  eta: 1:07:56  lr: 0.000007  loss: 1.5271  time: 0.5467  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [1500/8855]  eta: 1:07:27  lr: 0.000007  loss: 1.1778  time: 0.5469  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [1550/8855]  eta: 1:06:58  lr: 0.000007  loss: 1.4816  time: 0.5464  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [1600/8855]  eta: 1:06:31  lr: 0.000007  loss: 1.2805  time: 0.5495  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [1650/8855]  eta: 1:06:02  lr: 0.000007  loss: 1.2299  time: 0.5478  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [1700/8855]  eta: 1:05:34  lr: 0.000007  loss: 1.3559  time: 0.5524  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [1750/8855]  eta: 1:05:05  lr: 0.000007  loss: 1.1126  time: 0.5430  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [1800/8855]  eta: 1:04:37  lr: 0.000007  loss: 1.4999  time: 0.5426  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [1850/8855]  eta: 1:04:08  lr: 0.000007  loss: 1.5051  time: 0.5465  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [1900/8855]  eta: 1:03:41  lr: 0.000007  loss: 1.1878  time: 0.5473  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [1950/8855]  eta: 1:03:13  lr: 0.000007  loss: 1.4862  time: 0.5442  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [2000/8855]  eta: 1:02:45  lr: 0.000007  loss: 1.3735  time: 0.5480  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [2050/8855]  eta: 1:02:18  lr: 0.000007  loss: 1.4215  time: 0.5458  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [2100/8855]  eta: 1:01:50  lr: 0.000007  loss: 1.4142  time: 0.5444  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [2150/8855]  eta: 1:01:22  lr: 0.000007  loss: 1.2800  time: 0.5477  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [2200/8855]  eta: 1:00:53  lr: 0.000007  loss: 1.5989  time: 0.5435  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [2250/8855]  eta: 1:00:25  lr: 0.000007  loss: 1.4236  time: 0.5424  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [2300/8855]  eta: 0:59:58  lr: 0.000007  loss: 1.5836  time: 0.5492  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [2350/8855]  eta: 0:59:30  lr: 0.000007  loss: 1.3215  time: 0.5433  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [2400/8855]  eta: 0:59:02  lr: 0.000007  loss: 1.6259  time: 0.5482  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [2450/8855]  eta: 0:58:34  lr: 0.000007  loss: 1.2325  time: 0.5427  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [2500/8855]  eta: 0:58:06  lr: 0.000007  loss: 1.2685  time: 0.5433  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [2550/8855]  eta: 0:57:38  lr: 0.000007  loss: 1.3256  time: 0.5457  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [2600/8855]  eta: 0:57:11  lr: 0.000007  loss: 1.4757  time: 0.5470  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [2650/8855]  eta: 0:56:44  lr: 0.000007  loss: 1.5449  time: 0.5508  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [2700/8855]  eta: 0:56:17  lr: 0.000007  loss: 1.3712  time: 0.5589  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [2750/8855]  eta: 0:55:49  lr: 0.000007  loss: 1.2558  time: 0.5481  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [2800/8855]  eta: 0:55:20  lr: 0.000007  loss: 1.3444  time: 0.5457  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [2850/8855]  eta: 0:54:53  lr: 0.000007  loss: 1.4441  time: 0.5440  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [2900/8855]  eta: 0:54:25  lr: 0.000007  loss: 1.1702  time: 0.5458  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [2950/8855]  eta: 0:53:58  lr: 0.000007  loss: 1.6556  time: 0.5498  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [3000/8855]  eta: 0:53:30  lr: 0.000007  loss: 1.3317  time: 0.5402  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [3050/8855]  eta: 0:53:03  lr: 0.000007  loss: 1.5060  time: 0.5536  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [3100/8855]  eta: 0:52:35  lr: 0.000007  loss: 1.4540  time: 0.5495  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [3150/8855]  eta: 0:52:07  lr: 0.000007  loss: 1.3210  time: 0.5451  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [3200/8855]  eta: 0:51:40  lr: 0.000007  loss: 1.3225  time: 0.5469  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [3250/8855]  eta: 0:51:12  lr: 0.000007  loss: 1.5146  time: 0.5534  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [3300/8855]  eta: 0:50:45  lr: 0.000007  loss: 1.2028  time: 0.5468  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [3350/8855]  eta: 0:50:17  lr: 0.000007  loss: 1.3024  time: 0.5435  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [3400/8855]  eta: 0:49:50  lr: 0.000007  loss: 1.3639  time: 0.5454  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [3450/8855]  eta: 0:49:22  lr: 0.000007  loss: 1.5156  time: 0.5497  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [3500/8855]  eta: 0:48:55  lr: 0.000007  loss: 1.5831  time: 0.5452  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [3550/8855]  eta: 0:48:28  lr: 0.000007  loss: 1.4171  time: 0.5518  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [3600/8855]  eta: 0:48:01  lr: 0.000007  loss: 1.5533  time: 0.5526  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [3650/8855]  eta: 0:47:33  lr: 0.000007  loss: 1.2721  time: 0.5468  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [3700/8855]  eta: 0:47:06  lr: 0.000007  loss: 1.6291  time: 0.5536  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [3750/8855]  eta: 0:46:38  lr: 0.000007  loss: 1.4097  time: 0.5441  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [3800/8855]  eta: 0:46:11  lr: 0.000007  loss: 1.5123  time: 0.5523  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [3850/8855]  eta: 0:45:43  lr: 0.000007  loss: 1.1902  time: 0.5380  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [3900/8855]  eta: 0:45:16  lr: 0.000007  loss: 1.3636  time: 0.5496  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [3950/8855]  eta: 0:44:48  lr: 0.000007  loss: 1.2975  time: 0.5462  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [4000/8855]  eta: 0:44:21  lr: 0.000007  loss: 1.5482  time: 0.5552  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [4050/8855]  eta: 0:43:54  lr: 0.000007  loss: 1.4051  time: 0.5520  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [4100/8855]  eta: 0:43:26  lr: 0.000007  loss: 1.2817  time: 0.5372  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [4150/8855]  eta: 0:42:58  lr: 0.000007  loss: 1.4444  time: 0.5431  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [4200/8855]  eta: 0:42:30  lr: 0.000007  loss: 1.1778  time: 0.5429  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [4250/8855]  eta: 0:42:03  lr: 0.000007  loss: 1.2876  time: 0.5440  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [4300/8855]  eta: 0:41:36  lr: 0.000007  loss: 1.5026  time: 0.5478  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [4350/8855]  eta: 0:41:08  lr: 0.000007  loss: 1.4817  time: 0.5486  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [4400/8855]  eta: 0:40:41  lr: 0.000007  loss: 1.4273  time: 0.5511  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [4450/8855]  eta: 0:40:13  lr: 0.000007  loss: 1.2857  time: 0.5495  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [4500/8855]  eta: 0:39:46  lr: 0.000007  loss: 1.2235  time: 0.5425  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [4550/8855]  eta: 0:39:19  lr: 0.000007  loss: 1.4510  time: 0.5493  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [4600/8855]  eta: 0:38:51  lr: 0.000007  loss: 1.5031  time: 0.5523  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [4650/8855]  eta: 0:38:24  lr: 0.000007  loss: 1.0739  time: 0.5556  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [4700/8855]  eta: 0:37:56  lr: 0.000007  loss: 1.2765  time: 0.5387  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [4750/8855]  eta: 0:37:29  lr: 0.000007  loss: 1.5008  time: 0.5424  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [4800/8855]  eta: 0:37:01  lr: 0.000007  loss: 1.3534  time: 0.5468  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [4850/8855]  eta: 0:36:34  lr: 0.000007  loss: 1.2188  time: 0.5406  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [4900/8855]  eta: 0:36:06  lr: 0.000007  loss: 1.6936  time: 0.5421  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [4950/8855]  eta: 0:35:39  lr: 0.000007  loss: 1.0049  time: 0.5465  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [5000/8855]  eta: 0:35:12  lr: 0.000007  loss: 1.2415  time: 0.5526  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [5050/8855]  eta: 0:34:44  lr: 0.000007  loss: 1.2060  time: 0.5600  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [5100/8855]  eta: 0:34:17  lr: 0.000007  loss: 1.4932  time: 0.5485  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [5150/8855]  eta: 0:33:50  lr: 0.000007  loss: 1.1423  time: 0.5506  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [5200/8855]  eta: 0:33:22  lr: 0.000007  loss: 1.4483  time: 0.5480  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [5250/8855]  eta: 0:32:55  lr: 0.000007  loss: 1.3167  time: 0.5561  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [5300/8855]  eta: 0:32:28  lr: 0.000007  loss: 1.3141  time: 0.5600  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [5350/8855]  eta: 0:32:01  lr: 0.000007  loss: 1.0809  time: 0.5528  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [5400/8855]  eta: 0:31:33  lr: 0.000007  loss: 1.4960  time: 0.5513  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [5450/8855]  eta: 0:31:06  lr: 0.000007  loss: 1.2533  time: 0.5571  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [5500/8855]  eta: 0:30:39  lr: 0.000007  loss: 1.1880  time: 0.5510  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [5550/8855]  eta: 0:30:11  lr: 0.000007  loss: 1.7063  time: 0.5483  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [5600/8855]  eta: 0:29:44  lr: 0.000007  loss: 1.5612  time: 0.5554  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [5650/8855]  eta: 0:29:17  lr: 0.000007  loss: 1.5165  time: 0.5457  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [5700/8855]  eta: 0:28:49  lr: 0.000007  loss: 1.4750  time: 0.5508  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [5750/8855]  eta: 0:28:22  lr: 0.000007  loss: 1.3607  time: 0.5465  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [5800/8855]  eta: 0:27:55  lr: 0.000007  loss: 1.5104  time: 0.5435  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [5850/8855]  eta: 0:27:27  lr: 0.000007  loss: 1.5351  time: 0.5441  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [5900/8855]  eta: 0:27:00  lr: 0.000007  loss: 1.4321  time: 0.5464  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [5950/8855]  eta: 0:26:32  lr: 0.000007  loss: 1.2956  time: 0.5441  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [6000/8855]  eta: 0:26:05  lr: 0.000007  loss: 1.4202  time: 0.5445  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [6050/8855]  eta: 0:25:37  lr: 0.000007  loss: 1.5186  time: 0.5436  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [6100/8855]  eta: 0:25:10  lr: 0.000007  loss: 1.3745  time: 0.5438  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [6150/8855]  eta: 0:24:42  lr: 0.000007  loss: 1.3952  time: 0.5455  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [6200/8855]  eta: 0:24:15  lr: 0.000007  loss: 1.3497  time: 0.5477  data: 0.0000  max mem: 32954
Train: data epoch: [2]  [6250/8855]  eta: 0:23:48  lr: 0.000007  loss: 1.4384  time: 0.5421  data: 0.0000  max mem: 33077
Train: data epoch: [2]  [6300/8855]  eta: 0:23:20  lr: 0.000007  loss: 1.3955  time: 0.5396  data: 0.0000  max mem: 33077
Train: data epoch: [2]  [6350/8855]  eta: 0:22:53  lr: 0.000007  loss: 1.4087  time: 0.5424  data: 0.0000  max mem: 33077
Train: data epoch: [2]  [6400/8855]  eta: 0:22:25  lr: 0.000007  loss: 1.3367  time: 0.5494  data: 0.0000  max mem: 33077
Train: data epoch: [2]  [6450/8855]  eta: 0:21:58  lr: 0.000007  loss: 1.5324  time: 0.5431  data: 0.0000  max mem: 33077
Train: data epoch: [2]  [6500/8855]  eta: 0:21:30  lr: 0.000007  loss: 1.3919  time: 0.5433  data: 0.0000  max mem: 33077
Train: data epoch: [2]  [6550/8855]  eta: 0:21:03  lr: 0.000007  loss: 1.4720  time: 0.5446  data: 0.0000  max mem: 33077
Train: data epoch: [2]  [6600/8855]  eta: 0:20:35  lr: 0.000007  loss: 1.4940  time: 0.5467  data: 0.0000  max mem: 33077
Train: data epoch: [2]  [6650/8855]  eta: 0:20:08  lr: 0.000007  loss: 1.0749  time: 0.5523  data: 0.0000  max mem: 33077
Train: data epoch: [2]  [6700/8855]  eta: 0:19:41  lr: 0.000007  loss: 1.2571  time: 0.5415  data: 0.0000  max mem: 33077
Train: data epoch: [2]  [6750/8855]  eta: 0:19:13  lr: 0.000007  loss: 1.6580  time: 0.5471  data: 0.0000  max mem: 33077
Train: data epoch: [2]  [6800/8855]  eta: 0:18:46  lr: 0.000007  loss: 1.5356  time: 0.5445  data: 0.0000  max mem: 33077
Train: data epoch: [2]  [6850/8855]  eta: 0:18:18  lr: 0.000007  loss: 1.6858  time: 0.5509  data: 0.0000  max mem: 33077
Train: data epoch: [2]  [6900/8855]  eta: 0:17:51  lr: 0.000007  loss: 1.5332  time: 0.5419  data: 0.0000  max mem: 33077
Train: data epoch: [2]  [6950/8855]  eta: 0:17:23  lr: 0.000007  loss: 1.4628  time: 0.5435  data: 0.0000  max mem: 33077
Train: data epoch: [2]  [7000/8855]  eta: 0:16:56  lr: 0.000007  loss: 1.2388  time: 0.5405  data: 0.0000  max mem: 33077
Train: data epoch: [2]  [7050/8855]  eta: 0:16:28  lr: 0.000007  loss: 1.4841  time: 0.5482  data: 0.0000  max mem: 33077
Train: data epoch: [2]  [7100/8855]  eta: 0:16:01  lr: 0.000007  loss: 1.4456  time: 0.5471  data: 0.0000  max mem: 33077
Train: data epoch: [2]  [7150/8855]  eta: 0:15:34  lr: 0.000007  loss: 1.2486  time: 0.5510  data: 0.0000  max mem: 33077
Train: data epoch: [2]  [7200/8855]  eta: 0:15:06  lr: 0.000007  loss: 1.3782  time: 0.5453  data: 0.0000  max mem: 33077
Train: data epoch: [2]  [7250/8855]  eta: 0:14:39  lr: 0.000007  loss: 1.4332  time: 0.5429  data: 0.0000  max mem: 33077
Train: data epoch: [2]  [7300/8855]  eta: 0:14:11  lr: 0.000007  loss: 1.4357  time: 0.5423  data: 0.0000  max mem: 33077
Train: data epoch: [2]  [7350/8855]  eta: 0:13:44  lr: 0.000007  loss: 1.5136  time: 0.5507  data: 0.0000  max mem: 33077
Train: data epoch: [2]  [7400/8855]  eta: 0:13:17  lr: 0.000007  loss: 1.4906  time: 0.5409  data: 0.0000  max mem: 33077
Train: data epoch: [2]  [7450/8855]  eta: 0:12:49  lr: 0.000007  loss: 1.4818  time: 0.5455  data: 0.0000  max mem: 33077
Train: data epoch: [2]  [7500/8855]  eta: 0:12:22  lr: 0.000007  loss: 1.3903  time: 0.5465  data: 0.0000  max mem: 33077
Train: data epoch: [2]  [7550/8855]  eta: 0:11:54  lr: 0.000007  loss: 1.4603  time: 0.5498  data: 0.0000  max mem: 33077
Train: data epoch: [2]  [7600/8855]  eta: 0:11:27  lr: 0.000007  loss: 1.4406  time: 0.5583  data: 0.0000  max mem: 33077
Train: data epoch: [2]  [7650/8855]  eta: 0:11:00  lr: 0.000007  loss: 1.4265  time: 0.5502  data: 0.0000  max mem: 33077
Train: data epoch: [2]  [7700/8855]  eta: 0:10:32  lr: 0.000007  loss: 1.1751  time: 0.5479  data: 0.0000  max mem: 33077
Train: data epoch: [2]  [7750/8855]  eta: 0:10:05  lr: 0.000007  loss: 1.3656  time: 0.5566  data: 0.0000  max mem: 33077
Train: data epoch: [2]  [7800/8855]  eta: 0:09:37  lr: 0.000007  loss: 1.2151  time: 0.5460  data: 0.0000  max mem: 33077
Train: data epoch: [2]  [7850/8855]  eta: 0:09:10  lr: 0.000007  loss: 1.4798  time: 0.5515  data: 0.0000  max mem: 33077
Train: data epoch: [2]  [7900/8855]  eta: 0:08:43  lr: 0.000007  loss: 1.4094  time: 0.5387  data: 0.0000  max mem: 33077
Train: data epoch: [2]  [7950/8855]  eta: 0:08:15  lr: 0.000007  loss: 1.2487  time: 0.5421  data: 0.0000  max mem: 33077
Train: data epoch: [2]  [8000/8855]  eta: 0:07:48  lr: 0.000007  loss: 1.5532  time: 0.5371  data: 0.0000  max mem: 33077
Train: data epoch: [2]  [8050/8855]  eta: 0:07:20  lr: 0.000007  loss: 1.6858  time: 0.5438  data: 0.0000  max mem: 33077
Train: data epoch: [2]  [8100/8855]  eta: 0:06:53  lr: 0.000007  loss: 1.3452  time: 0.5545  data: 0.0000  max mem: 33077
Train: data epoch: [2]  [8150/8855]  eta: 0:06:26  lr: 0.000007  loss: 1.3497  time: 0.5479  data: 0.0000  max mem: 33077
Train: data epoch: [2]  [8200/8855]  eta: 0:05:58  lr: 0.000007  loss: 1.1970  time: 0.5562  data: 0.0000  max mem: 33077
Train: data epoch: [2]  [8250/8855]  eta: 0:05:31  lr: 0.000007  loss: 1.5781  time: 0.5470  data: 0.0000  max mem: 33077
Train: data epoch: [2]  [8300/8855]  eta: 0:05:04  lr: 0.000007  loss: 1.6242  time: 0.5461  data: 0.0000  max mem: 33077
Train: data epoch: [2]  [8350/8855]  eta: 0:04:36  lr: 0.000007  loss: 1.5758  time: 0.5471  data: 0.0000  max mem: 33077
Train: data epoch: [2]  [8400/8855]  eta: 0:04:09  lr: 0.000007  loss: 1.1795  time: 0.5509  data: 0.0000  max mem: 33077
Train: data epoch: [2]  [8450/8855]  eta: 0:03:41  lr: 0.000007  loss: 1.5116  time: 0.5464  data: 0.0000  max mem: 33077
Train: data epoch: [2]  [8500/8855]  eta: 0:03:14  lr: 0.000007  loss: 1.7000  time: 0.5451  data: 0.0000  max mem: 33077
Train: data epoch: [2]  [8550/8855]  eta: 0:02:47  lr: 0.000007  loss: 1.5096  time: 0.5435  data: 0.0000  max mem: 33077
Train: data epoch: [2]  [8600/8855]  eta: 0:02:19  lr: 0.000007  loss: 1.2330  time: 0.5499  data: 0.0000  max mem: 33077
Train: data epoch: [2]  [8650/8855]  eta: 0:01:52  lr: 0.000007  loss: 1.2671  time: 0.5389  data: 0.0000  max mem: 33077
Train: data epoch: [2]  [8700/8855]  eta: 0:01:24  lr: 0.000007  loss: 1.4855  time: 0.5416  data: 0.0000  max mem: 33077
Train: data epoch: [2]  [8750/8855]  eta: 0:00:57  lr: 0.000007  loss: 1.4696  time: 0.5507  data: 0.0000  max mem: 33077
Train: data epoch: [2]  [8800/8855]  eta: 0:00:30  lr: 0.000007  loss: 1.1847  time: 0.5465  data: 0.0000  max mem: 33077
Train: data epoch: [2]  [8850/8855]  eta: 0:00:02  lr: 0.000007  loss: 1.3096  time: 0.5458  data: 0.0000  max mem: 33077
Train: data epoch: [2]  [8854/8855]  eta: 0:00:00  lr: 0.000007  loss: 1.3611  time: 0.5550  data: 0.0000  max mem: 33077
Train: data epoch: [2] Total time: 1:20:50 (0.5478 s / it)
2023-05-31 00:17:35,259 [INFO] Averaged stats: lr: 0.0000  loss: 1.4090
2023-05-31 00:17:35,262 [INFO] Evaluating on val.
Evaluation  [  0/157]  eta: 0:05:48    time: 2.2217  data: 0.8582  max mem: 33077
Evaluation  [ 10/157]  eta: 0:03:38    time: 1.4894  data: 0.0793  max mem: 33077
Evaluation  [ 20/157]  eta: 0:03:16    time: 1.3985  data: 0.0014  max mem: 33077
Evaluation  [ 30/157]  eta: 0:03:00    time: 1.3834  data: 0.0014  max mem: 33077
Evaluation  [ 40/157]  eta: 0:02:44    time: 1.3715  data: 0.0014  max mem: 33077
Evaluation  [ 50/157]  eta: 0:02:29    time: 1.3522  data: 0.0014  max mem: 33077
Evaluation  [ 60/157]  eta: 0:02:14    time: 1.3483  data: 0.0014  max mem: 33077
Evaluation  [ 70/157]  eta: 0:02:00    time: 1.3824  data: 0.0014  max mem: 33077
Evaluation  [ 80/157]  eta: 0:01:47    time: 1.4019  data: 0.0014  max mem: 33077
Evaluation  [ 90/157]  eta: 0:01:32    time: 1.3567  data: 0.0014  max mem: 33077
Evaluation  [100/157]  eta: 0:01:18    time: 1.3594  data: 0.0014  max mem: 33077
Evaluation  [110/157]  eta: 0:01:04    time: 1.3813  data: 0.0014  max mem: 33077
Evaluation  [120/157]  eta: 0:00:51    time: 1.3524  data: 0.0014  max mem: 33077
Evaluation  [130/157]  eta: 0:00:37    time: 1.3397  data: 0.0014  max mem: 33077
Evaluation  [140/157]  eta: 0:00:23    time: 1.3158  data: 0.0014  max mem: 33077
Evaluation  [150/157]  eta: 0:00:09    time: 1.3151  data: 0.0014  max mem: 33077
Evaluation  [156/157]  eta: 0:00:01    time: 1.3062  data: 0.0094  max mem: 33077
Evaluation Total time: 0:03:34 (1.3650 s / it)
2023-05-31 00:21:14,342 [WARNING] rank 0 starts merging results.
result file saved to /root/Documents/DEMOS/InstructBLIP/output/BLIP2_Instruct/Caption_coco/20230530200/result/val_epoch2.json
Using downloaded and verified file: /root/Documents/CACHE/lavis/coco_gt/coco_karpathy_val_gt.json
loading annotations into memory...
Done (t=0.02s)
creating index...
index created!
Loading and preparing results...
DONE (t=0.01s)
creating index...
index created!
tokenization...
PTBTokenizer tokenized 307821 tokens at 1234344.79 tokens per second.
PTBTokenizer tokenized 70249 tokens at 486044.87 tokens per second.
setting up scorers...
computing Bleu score...
{'testlen': 64851, 'reflen': 59152, 'guess': [64851, 59851, 54851, 49851], 'correct': [47182, 27002, 14300, 7316]}
ratio: 1.096345009467117
Bleu_1: 0.728
Bleu_2: 0.573
Bleu_3: 0.441
Bleu_4: 0.335
computing METEOR score...
METEOR: 0.316
computing Rouge score...
ROUGE_L: 0.566
computing CIDEr score...
CIDEr: 1.192
computing SPICE score...
WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.nustaq.serialization.FSTClazzInfo (file:/data/root/anaconda3/envs/lavis/lib/python3.8/site-packages/pycocoevalcap/spice/lib/fst-2.47.jar) to field java.lang.String.value
WARNING: Please consider reporting this to the maintainers of org.nustaq.serialization.FSTClazzInfo
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
Parsing reference captions
Parsing test captions
Initiating Stanford parsing pipeline
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize
[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse
[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... 
done [0.4 sec].
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner
Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.0 sec].
Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.4 sec].
Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.5 sec].
Threads( StanfordCoreNLP ) [9.583 seconds]
Warning: Nashorn engine is planned to be removed from a future JDK release
SPICE evaluation took: 18.62 s
SPICE: 0.256
Bleu_1: 0.728
Bleu_2: 0.573
Bleu_3: 0.441
Bleu_4: 0.335
METEOR: 0.316
ROUGE_L: 0.566
CIDEr: 1.192
SPICE: 0.256
2023-05-31 00:21:56,288 [INFO] Start training
2023-05-31 00:21:56,320 [INFO] Start training epoch 3, 8855 iters per inner epoch.
/root/anaconda3/envs/lavis/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
Train: data epoch: [3]  [   0/8855]  eta: 8:49:46  lr: 0.000003  loss: 1.3962  time: 3.5896  data: 0.0001  max mem: 33077
Train: data epoch: [3]  [  50/8855]  eta: 1:28:41  lr: 0.000003  loss: 1.2952  time: 0.5493  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [ 100/8855]  eta: 1:24:01  lr: 0.000003  loss: 1.3256  time: 0.5462  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [ 150/8855]  eta: 1:22:21  lr: 0.000003  loss: 1.3675  time: 0.5481  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [ 200/8855]  eta: 1:21:14  lr: 0.000003  loss: 1.5012  time: 0.5546  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [ 250/8855]  eta: 1:20:14  lr: 0.000003  loss: 1.2500  time: 0.5451  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [ 300/8855]  eta: 1:19:32  lr: 0.000003  loss: 1.5050  time: 0.5504  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [ 350/8855]  eta: 1:18:57  lr: 0.000003  loss: 1.4194  time: 0.5530  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [ 400/8855]  eta: 1:18:17  lr: 0.000003  loss: 1.2679  time: 0.5473  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [ 450/8855]  eta: 1:17:42  lr: 0.000003  loss: 1.4644  time: 0.5457  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [ 500/8855]  eta: 1:17:11  lr: 0.000003  loss: 1.4763  time: 0.5581  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [ 550/8855]  eta: 1:16:41  lr: 0.000003  loss: 1.4899  time: 0.5489  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [ 600/8855]  eta: 1:16:07  lr: 0.000003  loss: 1.4082  time: 0.5401  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [ 650/8855]  eta: 1:15:39  lr: 0.000003  loss: 1.2919  time: 0.5501  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [ 700/8855]  eta: 1:15:05  lr: 0.000003  loss: 1.3922  time: 0.5391  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [ 750/8855]  eta: 1:14:37  lr: 0.000003  loss: 1.1269  time: 0.5432  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [ 800/8855]  eta: 1:14:06  lr: 0.000003  loss: 1.4038  time: 0.5431  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [ 850/8855]  eta: 1:13:34  lr: 0.000003  loss: 1.2472  time: 0.5381  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [ 900/8855]  eta: 1:13:02  lr: 0.000003  loss: 1.2003  time: 0.5374  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [ 950/8855]  eta: 1:12:33  lr: 0.000003  loss: 1.1610  time: 0.5463  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [1000/8855]  eta: 1:12:05  lr: 0.000003  loss: 1.2872  time: 0.5485  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [1050/8855]  eta: 1:11:36  lr: 0.000003  loss: 1.1336  time: 0.5472  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [1100/8855]  eta: 1:11:08  lr: 0.000003  loss: 1.6171  time: 0.5538  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [1150/8855]  eta: 1:10:38  lr: 0.000003  loss: 1.2580  time: 0.5480  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [1200/8855]  eta: 1:10:09  lr: 0.000003  loss: 1.3477  time: 0.5402  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [1250/8855]  eta: 1:09:41  lr: 0.000003  loss: 1.1742  time: 0.5478  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [1300/8855]  eta: 1:09:11  lr: 0.000003  loss: 1.3918  time: 0.5513  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [1350/8855]  eta: 1:08:44  lr: 0.000003  loss: 1.3315  time: 0.5595  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [1400/8855]  eta: 1:08:16  lr: 0.000003  loss: 1.2467  time: 0.5518  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [1450/8855]  eta: 1:07:49  lr: 0.000003  loss: 1.4301  time: 0.5471  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [1500/8855]  eta: 1:07:21  lr: 0.000003  loss: 1.1955  time: 0.5556  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [1550/8855]  eta: 1:06:53  lr: 0.000003  loss: 1.3932  time: 0.5443  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [1600/8855]  eta: 1:06:25  lr: 0.000003  loss: 1.4190  time: 0.5432  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [1650/8855]  eta: 1:05:56  lr: 0.000003  loss: 1.4446  time: 0.5433  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [1700/8855]  eta: 1:05:29  lr: 0.000003  loss: 1.6161  time: 0.5527  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [1750/8855]  eta: 1:05:01  lr: 0.000003  loss: 1.2052  time: 0.5460  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [1800/8855]  eta: 1:04:33  lr: 0.000003  loss: 1.9749  time: 0.5463  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [1850/8855]  eta: 1:04:05  lr: 0.000003  loss: 1.3644  time: 0.5439  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [1900/8855]  eta: 1:03:37  lr: 0.000003  loss: 1.1418  time: 0.5483  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [1950/8855]  eta: 1:03:09  lr: 0.000003  loss: 1.1477  time: 0.5427  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [2000/8855]  eta: 1:02:41  lr: 0.000003  loss: 1.1313  time: 0.5464  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [2050/8855]  eta: 1:02:13  lr: 0.000003  loss: 1.3463  time: 0.5483  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [2100/8855]  eta: 1:01:45  lr: 0.000003  loss: 1.7544  time: 0.5424  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [2150/8855]  eta: 1:01:17  lr: 0.000003  loss: 1.3870  time: 0.5479  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [2200/8855]  eta: 1:00:49  lr: 0.000003  loss: 1.3790  time: 0.5459  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [2250/8855]  eta: 1:00:22  lr: 0.000003  loss: 1.4115  time: 0.5423  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [2300/8855]  eta: 0:59:55  lr: 0.000003  loss: 1.6260  time: 0.5411  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [2350/8855]  eta: 0:59:28  lr: 0.000003  loss: 1.4561  time: 0.5507  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [2400/8855]  eta: 0:59:00  lr: 0.000003  loss: 1.4863  time: 0.5483  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [2450/8855]  eta: 0:58:33  lr: 0.000003  loss: 1.4376  time: 0.5460  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [2500/8855]  eta: 0:58:06  lr: 0.000003  loss: 1.5408  time: 0.5544  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [2550/8855]  eta: 0:57:38  lr: 0.000003  loss: 1.5821  time: 0.5455  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [2600/8855]  eta: 0:57:11  lr: 0.000003  loss: 1.1937  time: 0.5486  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [2650/8855]  eta: 0:56:43  lr: 0.000003  loss: 1.3670  time: 0.5484  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [2700/8855]  eta: 0:56:16  lr: 0.000003  loss: 1.6544  time: 0.5449  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [2750/8855]  eta: 0:55:48  lr: 0.000003  loss: 1.6706  time: 0.5459  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [2800/8855]  eta: 0:55:21  lr: 0.000003  loss: 1.3436  time: 0.5468  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [2850/8855]  eta: 0:54:53  lr: 0.000003  loss: 1.3905  time: 0.5511  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [2900/8855]  eta: 0:54:26  lr: 0.000003  loss: 1.2937  time: 0.5432  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [2950/8855]  eta: 0:53:58  lr: 0.000003  loss: 1.0057  time: 0.5428  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [3000/8855]  eta: 0:53:31  lr: 0.000003  loss: 1.2756  time: 0.5450  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [3050/8855]  eta: 0:53:03  lr: 0.000003  loss: 1.3331  time: 0.5474  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [3100/8855]  eta: 0:52:36  lr: 0.000003  loss: 1.3889  time: 0.5521  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [3150/8855]  eta: 0:52:08  lr: 0.000003  loss: 1.2853  time: 0.5455  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [3200/8855]  eta: 0:51:40  lr: 0.000003  loss: 1.4803  time: 0.5435  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [3250/8855]  eta: 0:51:13  lr: 0.000003  loss: 1.2461  time: 0.5479  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [3300/8855]  eta: 0:50:45  lr: 0.000003  loss: 1.2185  time: 0.5491  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [3350/8855]  eta: 0:50:18  lr: 0.000003  loss: 1.4946  time: 0.5452  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [3400/8855]  eta: 0:49:50  lr: 0.000003  loss: 1.2650  time: 0.5436  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [3450/8855]  eta: 0:49:23  lr: 0.000003  loss: 1.5956  time: 0.5525  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [3500/8855]  eta: 0:48:55  lr: 0.000003  loss: 1.4329  time: 0.5469  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [3550/8855]  eta: 0:48:27  lr: 0.000003  loss: 1.3593  time: 0.5467  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [3600/8855]  eta: 0:48:00  lr: 0.000003  loss: 1.2917  time: 0.5487  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [3650/8855]  eta: 0:47:32  lr: 0.000003  loss: 1.6191  time: 0.5478  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [3700/8855]  eta: 0:47:05  lr: 0.000003  loss: 1.4301  time: 0.5419  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [3750/8855]  eta: 0:46:37  lr: 0.000003  loss: 1.4353  time: 0.5395  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [3800/8855]  eta: 0:46:09  lr: 0.000003  loss: 1.2829  time: 0.5457  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [3850/8855]  eta: 0:45:42  lr: 0.000003  loss: 1.4377  time: 0.5519  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [3900/8855]  eta: 0:45:14  lr: 0.000003  loss: 1.4044  time: 0.5463  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [3950/8855]  eta: 0:44:47  lr: 0.000003  loss: 1.5615  time: 0.5485  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [4000/8855]  eta: 0:44:20  lr: 0.000003  loss: 1.4892  time: 0.5477  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [4050/8855]  eta: 0:43:52  lr: 0.000003  loss: 1.3242  time: 0.5500  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [4100/8855]  eta: 0:43:25  lr: 0.000003  loss: 1.9390  time: 0.5443  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [4150/8855]  eta: 0:42:57  lr: 0.000003  loss: 1.0872  time: 0.5419  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [4200/8855]  eta: 0:42:30  lr: 0.000003  loss: 1.3998  time: 0.5475  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [4250/8855]  eta: 0:42:02  lr: 0.000003  loss: 1.3519  time: 0.5454  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [4300/8855]  eta: 0:41:35  lr: 0.000003  loss: 1.3214  time: 0.5453  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [4350/8855]  eta: 0:41:08  lr: 0.000003  loss: 1.4808  time: 0.5509  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [4400/8855]  eta: 0:40:40  lr: 0.000003  loss: 1.1646  time: 0.5463  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [4450/8855]  eta: 0:40:13  lr: 0.000003  loss: 1.3773  time: 0.5450  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [4500/8855]  eta: 0:39:46  lr: 0.000003  loss: 1.2470  time: 0.5454  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [4550/8855]  eta: 0:39:18  lr: 0.000003  loss: 1.7544  time: 0.5433  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [4600/8855]  eta: 0:38:51  lr: 0.000003  loss: 1.3507  time: 0.5477  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [4650/8855]  eta: 0:38:23  lr: 0.000003  loss: 1.4470  time: 0.5482  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [4700/8855]  eta: 0:37:56  lr: 0.000003  loss: 1.2107  time: 0.5415  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [4750/8855]  eta: 0:37:28  lr: 0.000003  loss: 1.2508  time: 0.5476  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [4800/8855]  eta: 0:37:01  lr: 0.000003  loss: 1.4459  time: 0.5401  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [4850/8855]  eta: 0:36:33  lr: 0.000003  loss: 1.4300  time: 0.5522  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [4900/8855]  eta: 0:36:06  lr: 0.000003  loss: 1.5519  time: 0.5403  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [4950/8855]  eta: 0:35:39  lr: 0.000003  loss: 1.4399  time: 0.5455  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [5000/8855]  eta: 0:35:11  lr: 0.000003  loss: 1.3157  time: 0.5529  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [5050/8855]  eta: 0:34:44  lr: 0.000003  loss: 1.4892  time: 0.5454  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [5100/8855]  eta: 0:34:16  lr: 0.000003  loss: 1.2742  time: 0.5447  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [5150/8855]  eta: 0:33:49  lr: 0.000003  loss: 1.2529  time: 0.5460  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [5200/8855]  eta: 0:33:22  lr: 0.000003  loss: 1.4047  time: 0.5510  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [5250/8855]  eta: 0:32:54  lr: 0.000003  loss: 1.6598  time: 0.5397  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [5300/8855]  eta: 0:32:27  lr: 0.000003  loss: 1.4363  time: 0.5608  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [5350/8855]  eta: 0:31:59  lr: 0.000003  loss: 1.4746  time: 0.5408  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [5400/8855]  eta: 0:31:32  lr: 0.000003  loss: 1.5439  time: 0.5456  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [5450/8855]  eta: 0:31:04  lr: 0.000003  loss: 1.0802  time: 0.5475  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [5500/8855]  eta: 0:30:37  lr: 0.000003  loss: 1.3009  time: 0.5546  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [5550/8855]  eta: 0:30:10  lr: 0.000003  loss: 1.3511  time: 0.5382  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [5600/8855]  eta: 0:29:42  lr: 0.000003  loss: 1.5292  time: 0.5504  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [5650/8855]  eta: 0:29:15  lr: 0.000003  loss: 1.2180  time: 0.5399  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [5700/8855]  eta: 0:28:47  lr: 0.000003  loss: 1.5914  time: 0.5469  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [5750/8855]  eta: 0:28:20  lr: 0.000003  loss: 1.3548  time: 0.5369  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [5800/8855]  eta: 0:27:52  lr: 0.000003  loss: 1.6798  time: 0.5429  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [5850/8855]  eta: 0:27:25  lr: 0.000003  loss: 1.2702  time: 0.5486  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [5900/8855]  eta: 0:26:58  lr: 0.000003  loss: 1.4203  time: 0.5459  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [5950/8855]  eta: 0:26:30  lr: 0.000003  loss: 1.1847  time: 0.5397  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [6000/8855]  eta: 0:26:03  lr: 0.000003  loss: 1.3253  time: 0.5432  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [6050/8855]  eta: 0:25:36  lr: 0.000003  loss: 1.4386  time: 0.5608  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [6100/8855]  eta: 0:25:08  lr: 0.000003  loss: 1.4022  time: 0.5440  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [6150/8855]  eta: 0:24:41  lr: 0.000003  loss: 1.5393  time: 0.5465  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [6200/8855]  eta: 0:24:13  lr: 0.000003  loss: 1.3181  time: 0.5478  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [6250/8855]  eta: 0:23:46  lr: 0.000003  loss: 1.3215  time: 0.5508  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [6300/8855]  eta: 0:23:19  lr: 0.000003  loss: 1.4993  time: 0.5494  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [6350/8855]  eta: 0:22:51  lr: 0.000003  loss: 1.2786  time: 0.5406  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [6400/8855]  eta: 0:22:24  lr: 0.000003  loss: 1.4533  time: 0.5418  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [6450/8855]  eta: 0:21:57  lr: 0.000003  loss: 1.3502  time: 0.5490  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [6500/8855]  eta: 0:21:29  lr: 0.000003  loss: 1.6183  time: 0.5494  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [6550/8855]  eta: 0:21:02  lr: 0.000003  loss: 1.3363  time: 0.5462  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [6600/8855]  eta: 0:20:34  lr: 0.000003  loss: 1.3743  time: 0.5449  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [6650/8855]  eta: 0:20:07  lr: 0.000003  loss: 1.3417  time: 0.5517  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [6700/8855]  eta: 0:19:40  lr: 0.000003  loss: 1.1586  time: 0.5426  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [6750/8855]  eta: 0:19:12  lr: 0.000003  loss: 1.3182  time: 0.5472  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [6800/8855]  eta: 0:18:45  lr: 0.000003  loss: 1.6987  time: 0.5471  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [6850/8855]  eta: 0:18:17  lr: 0.000003  loss: 1.2676  time: 0.5426  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [6900/8855]  eta: 0:17:50  lr: 0.000003  loss: 1.4645  time: 0.5436  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [6950/8855]  eta: 0:17:23  lr: 0.000003  loss: 1.4028  time: 0.5454  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [7000/8855]  eta: 0:16:55  lr: 0.000003  loss: 1.3316  time: 0.5537  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [7050/8855]  eta: 0:16:28  lr: 0.000003  loss: 1.3135  time: 0.5449  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [7100/8855]  eta: 0:16:00  lr: 0.000003  loss: 1.2863  time: 0.5456  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [7150/8855]  eta: 0:15:33  lr: 0.000003  loss: 1.2994  time: 0.5417  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [7200/8855]  eta: 0:15:06  lr: 0.000003  loss: 1.3872  time: 0.5443  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [7250/8855]  eta: 0:14:38  lr: 0.000003  loss: 1.4415  time: 0.5386  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [7300/8855]  eta: 0:14:11  lr: 0.000003  loss: 1.4525  time: 0.5423  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [7350/8855]  eta: 0:13:43  lr: 0.000003  loss: 1.2297  time: 0.5430  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [7400/8855]  eta: 0:13:16  lr: 0.000003  loss: 1.2995  time: 0.5469  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [7450/8855]  eta: 0:12:49  lr: 0.000003  loss: 1.4862  time: 0.5450  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [7500/8855]  eta: 0:12:21  lr: 0.000003  loss: 1.6088  time: 0.5468  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [7550/8855]  eta: 0:11:54  lr: 0.000003  loss: 1.3474  time: 0.5424  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [7600/8855]  eta: 0:11:26  lr: 0.000003  loss: 1.2138  time: 0.5447  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [7650/8855]  eta: 0:10:59  lr: 0.000003  loss: 1.4220  time: 0.5395  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [7700/8855]  eta: 0:10:32  lr: 0.000003  loss: 1.2681  time: 0.5492  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [7750/8855]  eta: 0:10:04  lr: 0.000003  loss: 1.4066  time: 0.5440  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [7800/8855]  eta: 0:09:37  lr: 0.000003  loss: 1.2086  time: 0.5467  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [7850/8855]  eta: 0:09:10  lr: 0.000003  loss: 1.0429  time: 0.5478  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [7900/8855]  eta: 0:08:42  lr: 0.000003  loss: 1.3015  time: 0.5446  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [7950/8855]  eta: 0:08:15  lr: 0.000003  loss: 1.3009  time: 0.5511  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [8000/8855]  eta: 0:07:48  lr: 0.000003  loss: 1.6514  time: 0.5490  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [8050/8855]  eta: 0:07:20  lr: 0.000003  loss: 1.6279  time: 0.5410  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [8100/8855]  eta: 0:06:53  lr: 0.000003  loss: 1.2169  time: 0.5436  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [8150/8855]  eta: 0:06:25  lr: 0.000003  loss: 1.3430  time: 0.5456  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [8200/8855]  eta: 0:05:58  lr: 0.000003  loss: 1.6377  time: 0.5509  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [8250/8855]  eta: 0:05:31  lr: 0.000003  loss: 1.4287  time: 0.5480  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [8300/8855]  eta: 0:05:03  lr: 0.000003  loss: 1.2265  time: 0.5384  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [8350/8855]  eta: 0:04:36  lr: 0.000003  loss: 1.3053  time: 0.5443  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [8400/8855]  eta: 0:04:09  lr: 0.000003  loss: 1.3337  time: 0.5475  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [8450/8855]  eta: 0:03:41  lr: 0.000003  loss: 1.4109  time: 0.5572  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [8500/8855]  eta: 0:03:14  lr: 0.000003  loss: 1.3395  time: 0.5442  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [8550/8855]  eta: 0:02:46  lr: 0.000003  loss: 1.4775  time: 0.5471  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [8600/8855]  eta: 0:02:19  lr: 0.000003  loss: 1.4311  time: 0.5561  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [8650/8855]  eta: 0:01:52  lr: 0.000003  loss: 1.5977  time: 0.5501  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [8700/8855]  eta: 0:01:24  lr: 0.000003  loss: 1.4101  time: 0.5487  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [8750/8855]  eta: 0:00:57  lr: 0.000003  loss: 1.4804  time: 0.5497  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [8800/8855]  eta: 0:00:30  lr: 0.000003  loss: 1.4590  time: 0.5479  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [8850/8855]  eta: 0:00:02  lr: 0.000003  loss: 1.3713  time: 0.5512  data: 0.0000  max mem: 33077
Train: data epoch: [3]  [8854/8855]  eta: 0:00:00  lr: 0.000003  loss: 1.2530  time: 0.5582  data: 0.0000  max mem: 33077
Train: data epoch: [3] Total time: 1:20:48 (0.5475 s / it)
2023-05-31 01:42:44,697 [INFO] Averaged stats: lr: 0.0000  loss: 1.3941
2023-05-31 01:42:44,700 [INFO] Evaluating on val.
Evaluation  [  0/157]  eta: 0:05:46    time: 2.2098  data: 0.8367  max mem: 33077
Evaluation  [ 10/157]  eta: 0:03:35    time: 1.4647  data: 0.0773  max mem: 33077
Evaluation  [ 20/157]  eta: 0:03:15    time: 1.3893  data: 0.0014  max mem: 33077
Evaluation  [ 30/157]  eta: 0:02:59    time: 1.3851  data: 0.0014  max mem: 33077
Evaluation  [ 40/157]  eta: 0:02:44    time: 1.3763  data: 0.0014  max mem: 33077
Evaluation  [ 50/157]  eta: 0:02:28    time: 1.3538  data: 0.0014  max mem: 33077
Evaluation  [ 60/157]  eta: 0:02:13    time: 1.3330  data: 0.0014  max mem: 33077
Evaluation  [ 70/157]  eta: 0:01:59    time: 1.3466  data: 0.0014  max mem: 33077
Evaluation  [ 80/157]  eta: 0:01:46    time: 1.3722  data: 0.0014  max mem: 33077
Evaluation  [ 90/157]  eta: 0:01:32    time: 1.3626  data: 0.0014  max mem: 33077
Evaluation  [100/157]  eta: 0:01:18    time: 1.3664  data: 0.0014  max mem: 33077
Evaluation  [110/157]  eta: 0:01:04    time: 1.3659  data: 0.0014  max mem: 33077
Evaluation  [120/157]  eta: 0:00:50    time: 1.3426  data: 0.0014  max mem: 33077
Evaluation  [130/157]  eta: 0:00:36    time: 1.3476  data: 0.0014  max mem: 33077
Evaluation  [140/157]  eta: 0:00:23    time: 1.3478  data: 0.0014  max mem: 33077
Evaluation  [150/157]  eta: 0:00:09    time: 1.3403  data: 0.0014  max mem: 33077
Evaluation  [156/157]  eta: 0:00:01    time: 1.3124  data: 0.0093  max mem: 33077
Evaluation Total time: 0:03:33 (1.3619 s / it)
2023-05-31 01:46:22,291 [WARNING] rank 0 starts merging results.
result file saved to /root/Documents/DEMOS/InstructBLIP/output/BLIP2_Instruct/Caption_coco/20230530200/result/val_epoch3.json
Using downloaded and verified file: /root/Documents/CACHE/lavis/coco_gt/coco_karpathy_val_gt.json
loading annotations into memory...
Done (t=0.02s)
creating index...
index created!
Loading and preparing results...
DONE (t=0.01s)
creating index...
index created!
tokenization...
PTBTokenizer tokenized 307821 tokens at 1221440.05 tokens per second.
PTBTokenizer tokenized 70721 tokens at 501819.94 tokens per second.
setting up scorers...
computing Bleu score...
{'testlen': 65329, 'reflen': 59386, 'guess': [65329, 60329, 55329, 50329], 'correct': [47468, 27199, 14409, 7395]}
ratio: 1.1000740915367073
Bleu_1: 0.727
Bleu_2: 0.572
Bleu_3: 0.440
Bleu_4: 0.335
computing METEOR score...
METEOR: 0.318
computing Rouge score...
ROUGE_L: 0.565
computing CIDEr score...
CIDEr: 1.192
computing SPICE score...
WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.nustaq.serialization.FSTClazzInfo (file:/data/root/anaconda3/envs/lavis/lib/python3.8/site-packages/pycocoevalcap/spice/lib/fst-2.47.jar) to field java.lang.String.value
WARNING: Please consider reporting this to the maintainers of org.nustaq.serialization.FSTClazzInfo
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
Parsing reference captions
Parsing test captions
Initiating Stanford parsing pipeline
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize
[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse
[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... 
done [0.4 sec].
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner
Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [0.9 sec].
Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.7 sec].
Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.5 sec].
Threads( StanfordCoreNLP ) [5.609 seconds]
Warning: Nashorn engine is planned to be removed from a future JDK release
SPICE evaluation took: 15.44 s
SPICE: 0.256
Bleu_1: 0.727
Bleu_2: 0.572
Bleu_3: 0.440
Bleu_4: 0.335
METEOR: 0.318
ROUGE_L: 0.565
CIDEr: 1.192
SPICE: 0.256
2023-05-31 01:47:00,355 [INFO] Start training
2023-05-31 01:47:00,384 [INFO] Start training epoch 4, 8855 iters per inner epoch.
/root/anaconda3/envs/lavis/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
Train: data epoch: [4]  [   0/8855]  eta: 8:56:46  lr: 0.000001  loss: 1.5401  time: 3.6371  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [  50/8855]  eta: 1:29:19  lr: 0.000001  loss: 1.3570  time: 0.5464  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [ 100/8855]  eta: 1:24:16  lr: 0.000001  loss: 1.4594  time: 0.5450  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [ 150/8855]  eta: 1:22:24  lr: 0.000001  loss: 1.3032  time: 0.5503  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [ 200/8855]  eta: 1:21:03  lr: 0.000001  loss: 1.2757  time: 0.5397  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [ 250/8855]  eta: 1:20:06  lr: 0.000001  loss: 1.3267  time: 0.5402  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [ 300/8855]  eta: 1:19:27  lr: 0.000001  loss: 1.4933  time: 0.5525  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [ 350/8855]  eta: 1:18:45  lr: 0.000001  loss: 1.2511  time: 0.5448  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [ 400/8855]  eta: 1:18:06  lr: 0.000001  loss: 1.2123  time: 0.5450  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [ 450/8855]  eta: 1:17:26  lr: 0.000001  loss: 1.2556  time: 0.5439  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [ 500/8855]  eta: 1:16:57  lr: 0.000001  loss: 1.1948  time: 0.5586  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [ 550/8855]  eta: 1:16:25  lr: 0.000001  loss: 1.2070  time: 0.5463  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [ 600/8855]  eta: 1:15:58  lr: 0.000001  loss: 1.2792  time: 0.5602  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [ 650/8855]  eta: 1:15:27  lr: 0.000001  loss: 1.2575  time: 0.5520  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [ 700/8855]  eta: 1:14:56  lr: 0.000001  loss: 1.5104  time: 0.5409  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [ 750/8855]  eta: 1:14:24  lr: 0.000001  loss: 1.3209  time: 0.5445  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [ 800/8855]  eta: 1:13:52  lr: 0.000001  loss: 1.2780  time: 0.5438  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [ 850/8855]  eta: 1:13:23  lr: 0.000001  loss: 1.3573  time: 0.5488  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [ 900/8855]  eta: 1:12:52  lr: 0.000001  loss: 1.2864  time: 0.5374  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [ 950/8855]  eta: 1:12:24  lr: 0.000001  loss: 1.3158  time: 0.5530  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [1000/8855]  eta: 1:11:57  lr: 0.000001  loss: 1.4731  time: 0.5517  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [1050/8855]  eta: 1:11:27  lr: 0.000001  loss: 1.2072  time: 0.5469  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [1100/8855]  eta: 1:11:00  lr: 0.000001  loss: 1.5747  time: 0.5601  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [1150/8855]  eta: 1:10:31  lr: 0.000001  loss: 1.3697  time: 0.5474  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [1200/8855]  eta: 1:10:04  lr: 0.000001  loss: 1.4806  time: 0.5595  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [1250/8855]  eta: 1:09:37  lr: 0.000001  loss: 1.4604  time: 0.5485  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [1300/8855]  eta: 1:09:07  lr: 0.000001  loss: 1.5520  time: 0.5428  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [1350/8855]  eta: 1:08:41  lr: 0.000001  loss: 1.2453  time: 0.5561  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [1400/8855]  eta: 1:08:13  lr: 0.000001  loss: 1.4144  time: 0.5490  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [1450/8855]  eta: 1:07:44  lr: 0.000001  loss: 1.4263  time: 0.5458  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [1500/8855]  eta: 1:07:16  lr: 0.000001  loss: 1.4824  time: 0.5483  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [1550/8855]  eta: 1:06:49  lr: 0.000001  loss: 1.3585  time: 0.5526  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [1600/8855]  eta: 1:06:20  lr: 0.000001  loss: 1.3340  time: 0.5474  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [1650/8855]  eta: 1:05:51  lr: 0.000001  loss: 1.1198  time: 0.5493  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [1700/8855]  eta: 1:05:24  lr: 0.000001  loss: 1.1891  time: 0.5470  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [1750/8855]  eta: 1:04:57  lr: 0.000001  loss: 1.1938  time: 0.5597  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [1800/8855]  eta: 1:04:31  lr: 0.000001  loss: 1.5630  time: 0.5485  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [1850/8855]  eta: 1:04:02  lr: 0.000001  loss: 1.0895  time: 0.5449  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [1900/8855]  eta: 1:03:34  lr: 0.000001  loss: 1.2796  time: 0.5406  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [1950/8855]  eta: 1:03:07  lr: 0.000001  loss: 1.2369  time: 0.5497  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [2000/8855]  eta: 1:02:39  lr: 0.000001  loss: 1.6316  time: 0.5490  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [2050/8855]  eta: 1:02:11  lr: 0.000001  loss: 1.2849  time: 0.5488  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [2100/8855]  eta: 1:01:43  lr: 0.000001  loss: 1.4683  time: 0.5478  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [2150/8855]  eta: 1:01:16  lr: 0.000001  loss: 1.2943  time: 0.5472  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [2200/8855]  eta: 1:00:49  lr: 0.000001  loss: 1.2165  time: 0.5488  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [2250/8855]  eta: 1:00:21  lr: 0.000001  loss: 1.3088  time: 0.5477  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [2300/8855]  eta: 0:59:53  lr: 0.000001  loss: 1.6525  time: 0.5444  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [2350/8855]  eta: 0:59:27  lr: 0.000001  loss: 1.2543  time: 0.5499  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [2400/8855]  eta: 0:58:59  lr: 0.000001  loss: 1.4104  time: 0.5430  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [2450/8855]  eta: 0:58:31  lr: 0.000001  loss: 1.2681  time: 0.5434  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [2500/8855]  eta: 0:58:04  lr: 0.000001  loss: 1.7485  time: 0.5480  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [2550/8855]  eta: 0:57:36  lr: 0.000001  loss: 1.2981  time: 0.5453  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [2600/8855]  eta: 0:57:09  lr: 0.000001  loss: 1.4395  time: 0.5484  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [2650/8855]  eta: 0:56:41  lr: 0.000001  loss: 1.2548  time: 0.5425  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [2700/8855]  eta: 0:56:14  lr: 0.000001  loss: 1.5071  time: 0.5489  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [2750/8855]  eta: 0:55:46  lr: 0.000001  loss: 1.5148  time: 0.5436  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [2800/8855]  eta: 0:55:19  lr: 0.000001  loss: 1.3256  time: 0.5557  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [2850/8855]  eta: 0:54:51  lr: 0.000001  loss: 1.2049  time: 0.5404  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [2900/8855]  eta: 0:54:24  lr: 0.000001  loss: 1.5032  time: 0.5455  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [2950/8855]  eta: 0:53:57  lr: 0.000001  loss: 1.4644  time: 0.5430  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [3000/8855]  eta: 0:53:29  lr: 0.000001  loss: 1.2657  time: 0.5443  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [3050/8855]  eta: 0:53:02  lr: 0.000001  loss: 1.2053  time: 0.5477  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [3100/8855]  eta: 0:52:34  lr: 0.000001  loss: 1.6463  time: 0.5533  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [3150/8855]  eta: 0:52:07  lr: 0.000001  loss: 1.5592  time: 0.5549  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [3200/8855]  eta: 0:51:39  lr: 0.000001  loss: 1.3789  time: 0.5434  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [3250/8855]  eta: 0:51:11  lr: 0.000001  loss: 1.2209  time: 0.5425  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [3300/8855]  eta: 0:50:44  lr: 0.000001  loss: 1.4115  time: 0.5467  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [3350/8855]  eta: 0:50:16  lr: 0.000001  loss: 1.3049  time: 0.5469  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [3400/8855]  eta: 0:49:49  lr: 0.000001  loss: 1.3355  time: 0.5419  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [3450/8855]  eta: 0:49:22  lr: 0.000001  loss: 1.1638  time: 0.5570  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [3500/8855]  eta: 0:48:54  lr: 0.000001  loss: 1.5833  time: 0.5477  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [3550/8855]  eta: 0:48:27  lr: 0.000001  loss: 1.2465  time: 0.5540  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [3600/8855]  eta: 0:47:59  lr: 0.000001  loss: 1.4967  time: 0.5397  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [3650/8855]  eta: 0:47:31  lr: 0.000001  loss: 1.3538  time: 0.5397  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [3700/8855]  eta: 0:47:04  lr: 0.000001  loss: 1.5098  time: 0.5469  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [3750/8855]  eta: 0:46:36  lr: 0.000001  loss: 1.4579  time: 0.5487  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [3800/8855]  eta: 0:46:09  lr: 0.000001  loss: 1.3520  time: 0.5554  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [3850/8855]  eta: 0:45:42  lr: 0.000001  loss: 1.3611  time: 0.5459  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [3900/8855]  eta: 0:45:15  lr: 0.000001  loss: 1.2434  time: 0.5446  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [3950/8855]  eta: 0:44:47  lr: 0.000001  loss: 1.3230  time: 0.5485  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [4000/8855]  eta: 0:44:20  lr: 0.000001  loss: 1.5162  time: 0.5519  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [4050/8855]  eta: 0:43:53  lr: 0.000001  loss: 1.2233  time: 0.5543  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [4100/8855]  eta: 0:43:26  lr: 0.000001  loss: 1.3904  time: 0.5510  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [4150/8855]  eta: 0:42:58  lr: 0.000001  loss: 1.3464  time: 0.5611  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [4200/8855]  eta: 0:42:31  lr: 0.000001  loss: 1.3252  time: 0.5527  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [4250/8855]  eta: 0:42:04  lr: 0.000001  loss: 1.3775  time: 0.5492  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [4300/8855]  eta: 0:41:37  lr: 0.000001  loss: 1.3933  time: 0.5488  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [4350/8855]  eta: 0:41:09  lr: 0.000001  loss: 1.2469  time: 0.5475  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [4400/8855]  eta: 0:40:42  lr: 0.000001  loss: 1.3727  time: 0.5443  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [4450/8855]  eta: 0:40:14  lr: 0.000001  loss: 1.2906  time: 0.5483  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [4500/8855]  eta: 0:39:47  lr: 0.000001  loss: 1.3157  time: 0.5526  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [4550/8855]  eta: 0:39:20  lr: 0.000001  loss: 1.5101  time: 0.5465  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [4600/8855]  eta: 0:38:52  lr: 0.000001  loss: 1.2771  time: 0.5465  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [4650/8855]  eta: 0:38:25  lr: 0.000001  loss: 1.3455  time: 0.5528  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [4700/8855]  eta: 0:37:57  lr: 0.000001  loss: 1.2443  time: 0.5424  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [4750/8855]  eta: 0:37:30  lr: 0.000001  loss: 1.3786  time: 0.5554  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [4800/8855]  eta: 0:37:02  lr: 0.000001  loss: 1.3043  time: 0.5420  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [4850/8855]  eta: 0:36:35  lr: 0.000001  loss: 1.5057  time: 0.5449  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [4900/8855]  eta: 0:36:07  lr: 0.000001  loss: 1.5096  time: 0.5441  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [4950/8855]  eta: 0:35:40  lr: 0.000001  loss: 1.0684  time: 0.5414  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [5000/8855]  eta: 0:35:12  lr: 0.000001  loss: 1.7183  time: 0.5415  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [5050/8855]  eta: 0:34:45  lr: 0.000001  loss: 1.4305  time: 0.5439  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [5100/8855]  eta: 0:34:17  lr: 0.000001  loss: 1.0702  time: 0.5443  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [5150/8855]  eta: 0:33:50  lr: 0.000001  loss: 1.4928  time: 0.5532  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [5200/8855]  eta: 0:33:23  lr: 0.000001  loss: 1.3420  time: 0.5523  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [5250/8855]  eta: 0:32:55  lr: 0.000001  loss: 1.4788  time: 0.5465  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [5300/8855]  eta: 0:32:28  lr: 0.000001  loss: 1.1829  time: 0.5414  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [5350/8855]  eta: 0:32:01  lr: 0.000001  loss: 1.1447  time: 0.5445  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [5400/8855]  eta: 0:31:33  lr: 0.000001  loss: 1.3119  time: 0.5464  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [5450/8855]  eta: 0:31:06  lr: 0.000001  loss: 1.8074  time: 0.5450  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [5500/8855]  eta: 0:30:38  lr: 0.000001  loss: 1.8299  time: 0.5470  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [5550/8855]  eta: 0:30:11  lr: 0.000001  loss: 1.1660  time: 0.5531  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [5600/8855]  eta: 0:29:43  lr: 0.000001  loss: 1.2449  time: 0.5462  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [5650/8855]  eta: 0:29:16  lr: 0.000001  loss: 1.3864  time: 0.5465  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [5700/8855]  eta: 0:28:48  lr: 0.000001  loss: 1.3757  time: 0.5473  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [5750/8855]  eta: 0:28:21  lr: 0.000001  loss: 1.2485  time: 0.5600  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [5800/8855]  eta: 0:27:54  lr: 0.000001  loss: 1.6099  time: 0.5532  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [5850/8855]  eta: 0:27:26  lr: 0.000001  loss: 1.2721  time: 0.5448  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [5900/8855]  eta: 0:26:59  lr: 0.000001  loss: 1.0003  time: 0.5484  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [5950/8855]  eta: 0:26:31  lr: 0.000001  loss: 1.4408  time: 0.5558  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [6000/8855]  eta: 0:26:04  lr: 0.000001  loss: 1.5122  time: 0.5511  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [6050/8855]  eta: 0:25:37  lr: 0.000001  loss: 1.7417  time: 0.5413  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [6100/8855]  eta: 0:25:09  lr: 0.000001  loss: 1.4889  time: 0.5477  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [6150/8855]  eta: 0:24:42  lr: 0.000001  loss: 1.1733  time: 0.5494  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [6200/8855]  eta: 0:24:14  lr: 0.000001  loss: 1.3221  time: 0.5451  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [6250/8855]  eta: 0:23:47  lr: 0.000001  loss: 1.1944  time: 0.5512  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [6300/8855]  eta: 0:23:20  lr: 0.000001  loss: 1.4549  time: 0.5521  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [6350/8855]  eta: 0:22:52  lr: 0.000001  loss: 1.4271  time: 0.5506  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [6400/8855]  eta: 0:22:25  lr: 0.000001  loss: 1.1470  time: 0.5430  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [6450/8855]  eta: 0:21:58  lr: 0.000001  loss: 1.2732  time: 0.5420  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [6500/8855]  eta: 0:21:30  lr: 0.000001  loss: 1.6302  time: 0.5480  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [6550/8855]  eta: 0:21:03  lr: 0.000001  loss: 1.2133  time: 0.5481  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [6600/8855]  eta: 0:20:35  lr: 0.000001  loss: 1.5584  time: 0.5452  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [6650/8855]  eta: 0:20:08  lr: 0.000001  loss: 1.3288  time: 0.5463  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [6700/8855]  eta: 0:19:41  lr: 0.000001  loss: 1.3698  time: 0.5508  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [6750/8855]  eta: 0:19:13  lr: 0.000001  loss: 1.2715  time: 0.5466  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [6800/8855]  eta: 0:18:46  lr: 0.000001  loss: 1.3416  time: 0.5414  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [6850/8855]  eta: 0:18:18  lr: 0.000001  loss: 1.5081  time: 0.5476  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [6900/8855]  eta: 0:17:51  lr: 0.000001  loss: 1.3601  time: 0.5404  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [6950/8855]  eta: 0:17:23  lr: 0.000001  loss: 1.2078  time: 0.5462  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [7000/8855]  eta: 0:16:56  lr: 0.000001  loss: 1.6847  time: 0.5558  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [7050/8855]  eta: 0:16:29  lr: 0.000001  loss: 1.3835  time: 0.5480  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [7100/8855]  eta: 0:16:01  lr: 0.000001  loss: 1.2922  time: 0.5544  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [7150/8855]  eta: 0:15:34  lr: 0.000001  loss: 1.3849  time: 0.5521  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [7200/8855]  eta: 0:15:06  lr: 0.000001  loss: 1.3122  time: 0.5466  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [7250/8855]  eta: 0:14:39  lr: 0.000001  loss: 1.3584  time: 0.5398  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [7300/8855]  eta: 0:14:12  lr: 0.000001  loss: 1.5947  time: 0.5515  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [7350/8855]  eta: 0:13:44  lr: 0.000001  loss: 1.3292  time: 0.5450  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [7400/8855]  eta: 0:13:17  lr: 0.000001  loss: 1.4004  time: 0.5411  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [7450/8855]  eta: 0:12:49  lr: 0.000001  loss: 1.5963  time: 0.5522  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [7500/8855]  eta: 0:12:22  lr: 0.000001  loss: 1.1152  time: 0.5467  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [7550/8855]  eta: 0:11:55  lr: 0.000001  loss: 1.2260  time: 0.5473  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [7600/8855]  eta: 0:11:27  lr: 0.000001  loss: 1.3212  time: 0.5578  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [7650/8855]  eta: 0:11:00  lr: 0.000001  loss: 1.4509  time: 0.5448  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [7700/8855]  eta: 0:10:32  lr: 0.000001  loss: 1.1509  time: 0.5577  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [7750/8855]  eta: 0:10:05  lr: 0.000001  loss: 1.5156  time: 0.5491  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [7800/8855]  eta: 0:09:38  lr: 0.000001  loss: 1.2216  time: 0.5516  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [7850/8855]  eta: 0:09:10  lr: 0.000001  loss: 1.5949  time: 0.5494  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [7900/8855]  eta: 0:08:43  lr: 0.000001  loss: 1.3920  time: 0.5439  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [7950/8855]  eta: 0:08:15  lr: 0.000001  loss: 1.4744  time: 0.5484  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [8000/8855]  eta: 0:07:48  lr: 0.000001  loss: 1.4494  time: 0.5439  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [8050/8855]  eta: 0:07:21  lr: 0.000001  loss: 1.3755  time: 0.5512  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [8100/8855]  eta: 0:06:53  lr: 0.000001  loss: 1.5118  time: 0.5415  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [8150/8855]  eta: 0:06:26  lr: 0.000001  loss: 1.2972  time: 0.5429  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [8200/8855]  eta: 0:05:58  lr: 0.000001  loss: 1.2910  time: 0.5524  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [8250/8855]  eta: 0:05:31  lr: 0.000001  loss: 1.6869  time: 0.5538  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [8300/8855]  eta: 0:05:04  lr: 0.000001  loss: 1.4878  time: 0.5531  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [8350/8855]  eta: 0:04:36  lr: 0.000001  loss: 1.3014  time: 0.5427  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [8400/8855]  eta: 0:04:09  lr: 0.000001  loss: 1.4112  time: 0.5540  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [8450/8855]  eta: 0:03:41  lr: 0.000001  loss: 1.3375  time: 0.5431  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [8500/8855]  eta: 0:03:14  lr: 0.000001  loss: 1.5133  time: 0.5407  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [8550/8855]  eta: 0:02:47  lr: 0.000001  loss: 1.7315  time: 0.5503  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [8600/8855]  eta: 0:02:19  lr: 0.000001  loss: 1.4358  time: 0.5460  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [8650/8855]  eta: 0:01:52  lr: 0.000001  loss: 1.3773  time: 0.5430  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [8700/8855]  eta: 0:01:24  lr: 0.000001  loss: 1.3187  time: 0.5477  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [8750/8855]  eta: 0:00:57  lr: 0.000001  loss: 1.2735  time: 0.5457  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [8800/8855]  eta: 0:00:30  lr: 0.000001  loss: 1.2388  time: 0.5425  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [8850/8855]  eta: 0:00:02  lr: 0.000001  loss: 1.4870  time: 0.5437  data: 0.0000  max mem: 33077
Train: data epoch: [4]  [8854/8855]  eta: 0:00:00  lr: 0.000001  loss: 1.3021  time: 0.5512  data: 0.0000  max mem: 33077
Train: data epoch: [4] Total time: 1:20:51 (0.5479 s / it)
2023-05-31 03:07:52,180 [INFO] Averaged stats: lr: 0.0000  loss: 1.3846
2023-05-31 03:07:52,183 [INFO] Evaluating on val.
Evaluation  [  0/157]  eta: 0:05:48    time: 2.2189  data: 0.8423  max mem: 33077
Evaluation  [ 10/157]  eta: 0:03:37    time: 1.4779  data: 0.0778  max mem: 33077
Evaluation  [ 20/157]  eta: 0:03:15    time: 1.3896  data: 0.0014  max mem: 33077
Evaluation  [ 30/157]  eta: 0:02:59    time: 1.3752  data: 0.0014  max mem: 33077
Evaluation  [ 40/157]  eta: 0:02:43    time: 1.3637  data: 0.0014  max mem: 33077
Evaluation  [ 50/157]  eta: 0:02:28    time: 1.3600  data: 0.0014  max mem: 33077
Evaluation  [ 60/157]  eta: 0:02:14    time: 1.3738  data: 0.0014  max mem: 33077
Evaluation  [ 70/157]  eta: 0:02:00    time: 1.3734  data: 0.0014  max mem: 33077
Evaluation  [ 80/157]  eta: 0:01:46    time: 1.3733  data: 0.0014  max mem: 33077
Evaluation  [ 90/157]  eta: 0:01:33    time: 1.4066  data: 0.0014  max mem: 33077
Evaluation  [100/157]  eta: 0:01:19    time: 1.4009  data: 0.0014  max mem: 33077
Evaluation  [110/157]  eta: 0:01:05    time: 1.3641  data: 0.0014  max mem: 33077
Evaluation  [120/157]  eta: 0:00:51    time: 1.3384  data: 0.0014  max mem: 33077
Evaluation  [130/157]  eta: 0:00:37    time: 1.3277  data: 0.0014  max mem: 33077
Evaluation  [140/157]  eta: 0:00:23    time: 1.3353  data: 0.0014  max mem: 33077
Evaluation  [150/157]  eta: 0:00:09    time: 1.3422  data: 0.0014  max mem: 33077
Evaluation  [156/157]  eta: 0:00:01    time: 1.3320  data: 0.0094  max mem: 33077
Evaluation Total time: 0:03:35 (1.3703 s / it)
2023-05-31 03:11:27,348 [WARNING] rank 0 starts merging results.
result file saved to /root/Documents/DEMOS/InstructBLIP/output/BLIP2_Instruct/Caption_coco/20230530200/result/val_epoch4.json
Using downloaded and verified file: /root/Documents/CACHE/lavis/coco_gt/coco_karpathy_val_gt.json
loading annotations into memory...
Done (t=0.02s)
creating index...
index created!
Loading and preparing results...
DONE (t=0.16s)
creating index...
index created!
tokenization...
PTBTokenizer tokenized 307821 tokens at 1228159.98 tokens per second.
PTBTokenizer tokenized 70340 tokens at 489984.90 tokens per second.
setting up scorers...
computing Bleu score...
{'testlen': 64963, 'reflen': 59234, 'guess': [64963, 59963, 54963, 49963], 'correct': [47274, 26977, 14255, 7242]}
ratio: 1.0967181010905713
Bleu_1: 0.728
Bleu_2: 0.572
Bleu_3: 0.440
Bleu_4: 0.333
computing METEOR score...
METEOR: 0.316
computing Rouge score...
ROUGE_L: 0.564
computing CIDEr score...
CIDEr: 1.188
computing SPICE score...
WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.nustaq.serialization.FSTClazzInfo (file:/data/root/anaconda3/envs/lavis/lib/python3.8/site-packages/pycocoevalcap/spice/lib/fst-2.47.jar) to field java.lang.String.value
WARNING: Please consider reporting this to the maintainers of org.nustaq.serialization.FSTClazzInfo
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
Parsing reference captions
Parsing test captions
Initiating Stanford parsing pipeline
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize
[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse
[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... 
done [0.4 sec].
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner
Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.1 sec].
Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.5 sec].
Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.5 sec].
Threads( StanfordCoreNLP ) [5.107 seconds]
Warning: Nashorn engine is planned to be removed from a future JDK release
SPICE evaluation took: 14.28 s
SPICE: 0.256
Bleu_1: 0.728
Bleu_2: 0.572
Bleu_3: 0.440
Bleu_4: 0.333
METEOR: 0.316
ROUGE_L: 0.564
CIDEr: 1.188
SPICE: 0.256
2023-05-31 03:12:04,497 [INFO] Loading checkpoint from /root/Documents/DEMOS/InstructBLIP/output/BLIP2_Instruct/Caption_coco/20230530200/checkpoint_best.pth.
2023-05-31 03:12:05,995 [WARNING] 
                Key mismatch when loading checkpoint. This is expected if only part of the model is saved.
                Trying to load the model with strict=False.
                
2023-05-31 03:12:06,243 [WARNING] 
                Key mismatch when loading checkpoint. This is expected if only part of the model is saved.
                Trying to load the model with strict=False.
                
2023-05-31 03:12:06,514 [WARNING] 
                Key mismatch when loading checkpoint. This is expected if only part of the model is saved.
                Trying to load the model with strict=False.
                
2023-05-31 03:12:07,202 [WARNING] 
                Key mismatch when loading checkpoint. This is expected if only part of the model is saved.
                Trying to load the model with strict=False.
                
/root/anaconda3/envs/lavis/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
Evaluation  [  0/157]  eta: 0:06:14    time: 2.3871  data: 0.8466  max mem: 33077
Evaluation  [ 10/157]  eta: 0:03:31    time: 1.4366  data: 0.0783  max mem: 33077
Evaluation  [ 20/157]  eta: 0:03:13    time: 1.3632  data: 0.0014  max mem: 33077
Evaluation  [ 30/157]  eta: 0:02:57    time: 1.3785  data: 0.0014  max mem: 33077
Evaluation  [ 40/157]  eta: 0:02:41    time: 1.3530  data: 0.0014  max mem: 33077
Evaluation  [ 50/157]  eta: 0:02:27    time: 1.3502  data: 0.0014  max mem: 33077
Evaluation  [ 60/157]  eta: 0:02:13    time: 1.3518  data: 0.0014  max mem: 33077
Evaluation  [ 70/157]  eta: 0:01:59    time: 1.3542  data: 0.0014  max mem: 33077
Evaluation  [ 80/157]  eta: 0:01:44    time: 1.3222  data: 0.0014  max mem: 33077
Evaluation  [ 90/157]  eta: 0:01:30    time: 1.3057  data: 0.0014  max mem: 33077
Evaluation  [100/157]  eta: 0:01:17    time: 1.3314  data: 0.0014  max mem: 33077
Evaluation  [110/157]  eta: 0:01:03    time: 1.3699  data: 0.0014  max mem: 33077
Evaluation  [120/157]  eta: 0:00:50    time: 1.3632  data: 0.0014  max mem: 33077
Evaluation  [130/157]  eta: 0:00:36    time: 1.3263  data: 0.0014  max mem: 33077
Evaluation  [140/157]  eta: 0:00:23    time: 1.3465  data: 0.0014  max mem: 33077
Evaluation  [150/157]  eta: 0:00:09    time: 1.3571  data: 0.0014  max mem: 33077
Evaluation  [156/157]  eta: 0:00:01    time: 1.3426  data: 0.0092  max mem: 33077
Evaluation Total time: 0:03:32 (1.3537 s / it)
2023-05-31 03:15:39,878 [WARNING] rank 0 starts merging results.
result file saved to /root/Documents/DEMOS/InstructBLIP/output/BLIP2_Instruct/Caption_coco/20230530200/result/test_epochbest.json
Using downloaded and verified file: /root/Documents/CACHE/lavis/coco_gt/coco_karpathy_test_gt.json
loading annotations into memory...
Done (t=0.02s)
creating index...
index created!
Loading and preparing results...
DONE (t=0.01s)
creating index...
index created!
tokenization...
PTBTokenizer tokenized 307085 tokens at 1222909.26 tokens per second.
PTBTokenizer tokenized 69775 tokens at 486955.28 tokens per second.
setting up scorers...
computing Bleu score...
{'testlen': 64366, 'reflen': 58757, 'guess': [64366, 59366, 54366, 49366], 'correct': [46905, 27120, 14504, 7414]}
ratio: 1.0954609663529267
Bleu_1: 0.729
Bleu_2: 0.577
Bleu_3: 0.446
Bleu_4: 0.340
computing METEOR score...
METEOR: 0.317
computing Rouge score...
ROUGE_L: 0.569
computing CIDEr score...
CIDEr: 1.219
computing SPICE score...
WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.nustaq.serialization.FSTClazzInfo (file:/data/root/anaconda3/envs/lavis/lib/python3.8/site-packages/pycocoevalcap/spice/lib/fst-2.47.jar) to field java.lang.String.value
WARNING: Please consider reporting this to the maintainers of org.nustaq.serialization.FSTClazzInfo
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
Parsing reference captions
Parsing test captions
Initiating Stanford parsing pipeline
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize
[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse
[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... 
done [0.4 sec].
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner
Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.0 sec].
Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.4 sec].
Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.6 sec].
Threads( StanfordCoreNLP ) [11.706 seconds]
Warning: Nashorn engine is planned to be removed from a future JDK release
SPICE evaluation took: 20.74 s
SPICE: 0.260
Bleu_1: 0.729
Bleu_2: 0.577
Bleu_3: 0.446
Bleu_4: 0.340
METEOR: 0.317
ROUGE_L: 0.569
CIDEr: 1.219
SPICE: 0.260
2023-05-31 03:16:24,941 [INFO] Training time 7:10:17

WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
| distributed init (rank 1, world 2): env://
| distributed init (rank 0, world 2): env://
2023-06-04 14:19:28,991 [INFO] 
=====  Running Parameters    =====
2023-06-04 14:19:28,992 [INFO] {
    "batch_size_eval": 16,
    "batch_size_train": 32,
    "device": "cuda",
    "dist_backend": "nccl",
    "dist_url": "env://",
    "distributed": true,
    "evaluate": true,
    "gpu": 0,
    "max_len": 30,
    "min_len": 8,
    "num_beams": 5,
    "num_workers": 4,
    "output_dir": "/root/Documents/DEMOS/MiniGPT-4/X1A/output/Caption_coco/zeroshot",
    "rank": 0,
    "seed": 42,
    "task": "captioning",
    "test_splits": [
        "test"
    ],
    "world_size": 2
}
2023-06-04 14:19:28,992 [INFO] 
======  Dataset Attributes  ======
2023-06-04 14:19:28,992 [INFO] 
======== coco_caption =======
2023-06-04 14:19:28,993 [INFO] {
    "build_info": {
        "annotations": {
            "test": {
                "md5": "3ff34b0ef2db02d01c37399f6a2a6cd1",
                "storage": "/root/Documents/DATASETS/MS_COCO/annotations/coco_karpathy_test.json",
                "url": "https://storage.googleapis.com/sfr-vision-language-research/datasets/coco_karpathy_test.json"
            },
            "train": {
                "md5": "aa31ac474cf6250ebb81d18348a07ed8",
                "storage": "/root/Documents/DATASETS/MS_COCO/annotations/coco_karpathy_train.json",
                "url": "https://storage.googleapis.com/sfr-vision-language-research/datasets/coco_karpathy_train.json"
            },
            "val": {
                "md5": "b273847456ef5580e33713b1f7de52a0",
                "storage": "/root/Documents/DATASETS/MS_COCO/annotations/coco_karpathy_val.json",
                "url": "https://storage.googleapis.com/sfr-vision-language-research/datasets/coco_karpathy_val.json"
            }
        },
        "images": {
            "storage": "/root/Documents/DATASETS/MS_COCO/images/"
        }
    },
    "data_type": "images",
    "dataset_card": "dataset_card/coco_caption.md",
    "text_processor": {
        "eval": {
            "name": "blip_caption"
        }
    },
    "vis_processor": {
        "eval": {
            "image_size": 224,
            "name": "blip_image_eval"
        }
    }
}
2023-06-04 14:19:28,993 [INFO] 
======  Model Attributes  ======
2023-06-04 14:19:28,993 [INFO] {
    "arch": "mini_gpt4",
    "ckpt": "/root/Documents/MODELS/MiniGPT-4/7B/prerained_minigpt4_7b.pth",
    "drop_path_rate": 0,
    "end_sym": "###",
    "freeze_qformer": true,
    "freeze_vit": true,
    "image_size": 224,
    "llama_model": "/root/Documents/MODELS/Vicuna-V0/7B",
    "low_resource": false,
    "max_txt_len": 160,
    "model_type": "pretrain_vicuna_7B",
    "num_query_token": 32,
    "prompt": "",
    "prompt_path": "/root/Documents/DEMOS/MiniGPT-4/X1A/prompts/image_caption.txt",
    "prompt_template": "###Human: {} ###Assistant: ",
    "use_grad_checkpoint": false,
    "vit_precision": "fp16"
}
Using downloaded and verified file: /root/Documents/DATASETS/MS_COCO/annotations/coco_karpathy_train.json
Using downloaded and verified file: /root/Documents/DATASETS/MS_COCO/annotations/coco_karpathy_val.json
Using downloaded and verified file: /root/Documents/DATASETS/MS_COCO/annotations/coco_karpathy_test.json
2023-06-04 14:19:28,995 [INFO] Building datasets...
Loading VIT
2023-06-04 14:19:54,499 [INFO] freeze vision encoder
Loading VIT Done
Loading Q-Former
2023-06-04 14:19:58,623 [INFO] load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/BLIP2/blip2_pretrained_flant5xxl.pth
2023-06-04 14:19:58,640 [INFO] freeze Qformer
Loading Q-Former Done
Loading LLAMA
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.78s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.48s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.91s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.49s/it]
Loading LLAMA Done
Load 11 training prompts
Prompt Example 
###Human: <Img><ImageHere></Img> Use a few words to illustrate what is happening in the picture. ###Assistant: 
Load BLIP2-LLM Checkpoint: /root/Documents/MODELS/MiniGPT-4/7B/prerained_minigpt4_7b.pth
2023-06-04 14:22:18,556 [INFO] dataset_ratios not specified, datasets will be concatenated (map-style datasets) or chained (webdataset.DataPipeline).
2023-06-04 14:22:18,556 [INFO] Loaded 566747 records for train split from the dataset.
2023-06-04 14:22:18,556 [INFO] Loaded 5000 records for val split from the dataset.
2023-06-04 14:22:18,556 [INFO] Loaded 5000 records for test split from the dataset.
2023-06-04 14:22:18,556 [INFO] Empty train splits.
2023-06-04 14:22:18,556 [INFO] Empty train splits.
2023-06-04 14:22:18,557 [INFO] Empty train splits.
Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.36s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.98s/it]
Evaluation  [  0/157]  eta: 0:23:24    time: 8.9480  data: 5.3986  max mem: 25006
Evaluation  [ 10/157]  eta: 0:06:56    time: 2.8350  data: 0.4919  max mem: 25852
Evaluation  [ 20/157]  eta: 0:05:48    time: 2.2252  data: 0.0012  max mem: 26214
Evaluation  [ 30/157]  eta: 0:05:11    time: 2.2401  data: 0.0012  max mem: 26334
Evaluation  [ 40/157]  eta: 0:04:41    time: 2.2549  data: 0.0012  max mem: 26334
Evaluation  [ 50/157]  eta: 0:04:14    time: 2.2556  data: 0.0012  max mem: 26334
Evaluation  [ 60/157]  eta: 0:03:48    time: 2.2514  data: 0.0012  max mem: 26334
Evaluation  [ 70/157]  eta: 0:03:23    time: 2.2526  data: 0.0012  max mem: 26334
Evaluation  [ 80/157]  eta: 0:02:59    time: 2.2637  data: 0.0012  max mem: 26334
Evaluation  [ 90/157]  eta: 0:02:35    time: 2.2705  data: 0.0012  max mem: 26334
Evaluation  [100/157]  eta: 0:02:12    time: 2.2674  data: 0.0012  max mem: 26334
Evaluation  [110/157]  eta: 0:01:48    time: 2.2716  data: 0.0012  max mem: 26334
Evaluation  [120/157]  eta: 0:01:25    time: 2.3067  data: 0.0012  max mem: 26334
Evaluation  [130/157]  eta: 0:01:02    time: 2.2887  data: 0.0012  max mem: 26334
Evaluation  [140/157]  eta: 0:00:39    time: 2.2676  data: 0.0012  max mem: 26334
Evaluation  [150/157]  eta: 0:00:16    time: 2.2767  data: 0.0012  max mem: 26334
Evaluation  [156/157]  eta: 0:00:02    time: 2.2493  data: 0.0253  max mem: 26334
Evaluation Total time: 0:06:01 (2.3030 s / it)
2023-06-04 14:28:26,999 [WARNING] rank 0 starts merging results.
result file saved to /root/Documents/DEMOS/MiniGPT-4/X1A/output/Caption_coco/zeroshot/20230604141/result/test_epochbest.json
Using downloaded and verified file: /root/Documents/CACHE/minigpt4/coco_gt/coco_karpathy_test_gt.json
loading annotations into memory...
Done (t=0.02s)
creating index...
index created!
Loading and preparing results...
DONE (t=0.01s)
creating index...
index created!
tokenization...
PTBTokenizer tokenized 307085 tokens at 1243881.50 tokens per second.
Jun 04, 2023 2:28:28 PM edu.stanford.nlp.process.PTBLexer next
WARNING: Untokenizable: ? (U+D83C, decimal: 55356)
PTBTokenizer tokenized 127971 tokens at 593966.59 tokens per second.
setting up scorers...
computing Bleu score...
{'testlen': 115168, 'reflen': 65233, 'guess': [115168, 110211, 105254, 100297], 'correct': [55521, 26457, 11902, 5368]}
ratio: 1.7654867934940635
Bleu_1: 0.482
Bleu_2: 0.340
Bleu_3: 0.236
Bleu_4: 0.163
computing METEOR score...
METEOR: 0.272
computing Rouge score...
ROUGE_L: 0.409
computing CIDEr score...
CIDEr: 0.126
computing SPICE score...
WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.nustaq.serialization.FSTClazzInfo (file:/data/root/anaconda3/envs/minigpt4/lib/python3.9/site-packages/pycocoevalcap/spice/lib/fst-2.47.jar) to field java.lang.String.value
WARNING: Please consider reporting this to the maintainers of org.nustaq.serialization.FSTClazzInfo
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
Parsing reference captions
Parsing test captions
Initiating Stanford parsing pipeline
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize
[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse
[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... 
done [0.4 sec].
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner
Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [0.9 sec].
Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.4 sec].
Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.5 sec].
Threads( StanfordCoreNLP ) [42.616 seconds]
Warning: Nashorn engine is planned to be removed from a future JDK release
SPICE evaluation took: 54.35 s
SPICE: 0.221
Bleu_1: 0.482
Bleu_2: 0.340
Bleu_3: 0.236
Bleu_4: 0.163
METEOR: 0.272
ROUGE_L: 0.409
CIDEr: 0.126
SPICE: 0.221
